{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import regex as re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import BCELoss\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = './data'\n",
    "\n",
    "# create dataframe from sessions.json\n",
    "df = pd.read_json(f'{path_to_data}/sessions.json')\n",
    "df.head()\n",
    "\n",
    "# create dictionaries for switching between symptom and id\n",
    "id2sym = {}\n",
    "sym2id = {}\n",
    "\n",
    "with open(f'{path_to_data}/symptoms.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    for sym in data:\n",
    "        id2sym[sym['id']] = sym['name']\n",
    "        sym2id[sym['name']] = sym['id']\n",
    "        \n",
    "        \n",
    "# remove labels that have less than m occurrences\n",
    "m = 0\n",
    "\n",
    "labels_list = df['confirmed'].tolist()\n",
    "labels_list = sum(labels_list, [])\n",
    "c = Counter(labels_list)\n",
    "for i in range(len(df)):\n",
    "    to_remove = []\n",
    "    \n",
    "    # find labels that should be removed \n",
    "    for j in range(len(df['confirmed'][i])):\n",
    "        if c[df['confirmed'][i][j]] < m:\n",
    "            to_remove.append(j)\n",
    "            \n",
    "    # remove the labels\n",
    "    shift = 0\n",
    "    for j in range(len(to_remove)):\n",
    "        df['confirmed'][i].pop(to_remove[j]-shift)\n",
    "        shift += 1\n",
    "    \n",
    "        \n",
    "# add column with the symptom names\n",
    "sym_names = []\n",
    "\n",
    "for syms in df['confirmed']:\n",
    "    if len(syms) != 0:\n",
    "        sym_names.append([id2sym[x] for x in syms])\n",
    "    else:\n",
    "        sym_names.append([])\n",
    "\n",
    "df['labels'] = sym_names\n",
    "\n",
    "# remove all rows with no confirmed labels\n",
    "df = df[df['confirmed'].map(len) > 0]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Slut på medicin.</td>\n",
       "      <td>[Känd astma, Känd lungsjukdom]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Behöver att prata med psykolog angående använd...</td>\n",
       "      <td>[Nedstämdhet, Trötthet]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Har fått besvärlig eksem på händerna</td>\n",
       "      <td>[Hudbesvär, Synliga hudbesvär]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Muskelsvaghet och trötthet känner mig skakig o...</td>\n",
       "      <td>[Muskelsvaghet, Trötthet]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Svår smärta i vänsterhanden/handleden precis n...</td>\n",
       "      <td>[Smärta i handled eller fingrar, Förvärras av ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                                   Slut på medicin.   \n",
       "1  Behöver att prata med psykolog angående använd...   \n",
       "2              Har fått besvärlig eksem på händerna    \n",
       "3  Muskelsvaghet och trötthet känner mig skakig o...   \n",
       "4  Svår smärta i vänsterhanden/handleden precis n...   \n",
       "\n",
       "                                              labels  \n",
       "0                     [Känd astma, Känd lungsjukdom]  \n",
       "1                            [Nedstämdhet, Trötthet]  \n",
       "2                     [Hudbesvär, Synliga hudbesvär]  \n",
       "3                          [Muskelsvaghet, Trötthet]  \n",
       "4  [Smärta i handled eller fingrar, Förvärras av ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop('confirmed', inplace=True, axis=1)\n",
    "df.drop('suggested', inplace=True, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hej', 'hur', 'är', 'läget']\n"
     ]
    }
   ],
   "source": [
    "# a basic tokenizer to start off with\n",
    "def basic_tokenization(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    tokens = re.sub(r'[^\\p{L} ]', '', text).split()\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "test = 'Hej, hur är läget?'\n",
    "print(basic_tokenization(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[192250, 300]\n",
      "100000\n",
      "tensor([[-2.2332,  3.3888,  2.0338,  ...,  0.1474,  2.6168,  0.1402],\n",
      "        [ 2.1269, -1.5451,  1.0750,  ..., -1.2130,  0.4903,  1.5822],\n",
      "        [ 4.6350,  0.8348,  0.9662,  ..., -4.5444, -3.1763, -1.2721],\n",
      "        ...,\n",
      "        [ 0.0438,  0.6291, -0.9723,  ..., -0.5011, -0.1992, -0.6892],\n",
      "        [-0.9830, -1.0080,  0.4821,  ..., -0.2133,  0.6264,  0.1886],\n",
      "        [-0.2024, -0.7025, -0.3341,  ...,  0.1908,  0.1050,  0.7176]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# create dictionary that assigns a unique integer id for each word in the embeddings\n",
    "word2id = {}\n",
    "id_count = 0\n",
    "\n",
    "embedding_weights = None\n",
    "\n",
    "with open('./embeddings/swectors-300dim.txt', encoding='utf-8') as file:\n",
    "    # initialize the embedding weights matrix with zeros\n",
    "    dims = [int(x) for x in file.readline().split()]\n",
    "    print(dims)\n",
    "    embedding_weights = torch.zeros((dims[0],dims[1]), dtype=torch.float64)    \n",
    "    \n",
    "    line = file.readline().split()\n",
    "    while line != []:\n",
    "        word2id[line[0]] = id_count\n",
    "        embedding_weights[id_count,:] = torch.tensor([float(x) for x in line[1:]])\n",
    "        id_count += 1\n",
    "        \n",
    "        line = file.readline()\n",
    "        line = line.split()\n",
    "        \n",
    "        if id_count % 100000 == 0:\n",
    "            print(id_count)\n",
    "        \n",
    "print(embedding_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a multilabel_binarizer on the labels\n",
    "labels = df['labels'].tolist()\n",
    "multilab_bin = MultiLabelBinarizer()\n",
    "multilab_bin.fit(labels)\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, word2id, multilab_bin, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.word2id = word2id\n",
    "        self.multilab_bin = multilab_bin\n",
    "        self.data = dataframe\n",
    "        self.text = self.data['text']\n",
    "        self.labels = self.data['labels']\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        tokens = self.tokenizer(text)\n",
    "        ids = [self.word2id[w] for w in tokens]\n",
    "        \n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'labels': torch.tensor(np.sum(self.multilab_bin.transform([self.labels[index]]), axis=0), dtype=torch.float)\n",
    "        }      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "Train set: 3027 samples\n",
      "Test set: 757 samples\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# max number of tokens in text\n",
    "#max_len = 200\n",
    "max_len = 0\n",
    "for i in range(len(df['text'])):\n",
    "    text = df['text'][i]\n",
    "            \n",
    "    tokens = basic_tokenization(text)\n",
    "    \n",
    "    if len(tokens) > max_len:\n",
    "        max_len = len(tokens)\n",
    "\n",
    "print(max_len)\n",
    "\n",
    "train_dataset, test_dataset = train_test_split(df,\n",
    "                                        random_state=42,\n",
    "                                        test_size=0.2,\n",
    "                                        shuffle=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "test_dataset = test_dataset.reset_index(drop=True)\n",
    "\n",
    "train_set = CustomDataset(train_dataset, basic_tokenization, word2id, multilab_bin, max_len)\n",
    "test_set = CustomDataset(test_dataset, basic_tokenization, word2id, multilab_bin, max_len)\n",
    "\n",
    "train_params = {'batch_size': batch_size,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "               }\n",
    "test_params = {'batch_size': batch_size,\n",
    "               'shuffle': True,\n",
    "               'num_workers': 0\n",
    "              }\n",
    "\n",
    "train_loader = DataLoader(train_set, **train_params)\n",
    "test_loader = DataLoader(test_set, **test_params)\n",
    "\n",
    "print(f'Train set: {len(train_dataset)} samples')\n",
    "print(f'Test set: {len(test_dataset)} samples')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
