{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import regex as re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import BCELoss\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = './data'\n",
    "\n",
    "# create dataframe from sessions.json\n",
    "df = pd.read_json(f'{path_to_data}/sessions.json')\n",
    "df.head()\n",
    "\n",
    "# create dictionaries for switching between symptom and id\n",
    "id2sym = {}\n",
    "sym2id = {}\n",
    "\n",
    "with open(f'{path_to_data}/symptoms.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    for sym in data:\n",
    "        id2sym[sym['id']] = sym['name']\n",
    "        sym2id[sym['name']] = sym['id']\n",
    "        \n",
    "        \n",
    "# remove labels that have less than m occurrences\n",
    "m = 0\n",
    "\n",
    "labels_list = df['confirmed'].tolist()\n",
    "labels_list = sum(labels_list, [])\n",
    "c = Counter(labels_list)\n",
    "for i in range(len(df)):\n",
    "    to_remove = []\n",
    "    \n",
    "    # find labels that should be removed \n",
    "    for j in range(len(df['confirmed'][i])):\n",
    "        if c[df['confirmed'][i][j]] < m:\n",
    "            to_remove.append(j)\n",
    "            \n",
    "    # remove the labels\n",
    "    shift = 0\n",
    "    for j in range(len(to_remove)):\n",
    "        df['confirmed'][i].pop(to_remove[j]-shift)\n",
    "        shift += 1\n",
    "    \n",
    "        \n",
    "# add column with the symptom names\n",
    "sym_names = []\n",
    "\n",
    "for syms in df['confirmed']:\n",
    "    if len(syms) != 0:\n",
    "        sym_names.append([id2sym[x] for x in syms])\n",
    "    else:\n",
    "        sym_names.append([])\n",
    "\n",
    "df['labels'] = sym_names\n",
    "\n",
    "# remove all rows with no confirmed labels\n",
    "df = df[df['confirmed'].map(len) > 0]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Slut på medicin.</td>\n",
       "      <td>[Känd astma, Känd lungsjukdom]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Behöver att prata med psykolog angående använd...</td>\n",
       "      <td>[Nedstämdhet, Trötthet]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Har fått besvärlig eksem på händerna</td>\n",
       "      <td>[Hudbesvär, Synliga hudbesvär]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Muskelsvaghet och trötthet känner mig skakig o...</td>\n",
       "      <td>[Muskelsvaghet, Trötthet]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Svår smärta i vänsterhanden/handleden precis n...</td>\n",
       "      <td>[Smärta i handled eller fingrar, Förvärras av ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                                   Slut på medicin.   \n",
       "1  Behöver att prata med psykolog angående använd...   \n",
       "2              Har fått besvärlig eksem på händerna    \n",
       "3  Muskelsvaghet och trötthet känner mig skakig o...   \n",
       "4  Svår smärta i vänsterhanden/handleden precis n...   \n",
       "\n",
       "                                              labels  \n",
       "0                     [Känd astma, Känd lungsjukdom]  \n",
       "1                            [Nedstämdhet, Trötthet]  \n",
       "2                     [Hudbesvär, Synliga hudbesvär]  \n",
       "3                          [Muskelsvaghet, Trötthet]  \n",
       "4  [Smärta i handled eller fingrar, Förvärras av ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop('confirmed', inplace=True, axis=1)\n",
    "df.drop('suggested', inplace=True, axis=1)\n",
    "#df = df[0:100]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hej', 'hur', 'är', 'läget']\n"
     ]
    }
   ],
   "source": [
    "# a basic tokenizer to start off with\n",
    "def basic_tokenization(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    tokens = re.sub(r'[^\\p{L} ]', '', text).split()\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "test = 'Hej, hur är läget?'\n",
    "print(basic_tokenization(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[192250, 300]\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "# create dictionary that assigns a unique integer id for each word in the embeddings\n",
    "word2id = {}\n",
    "id_count = 0\n",
    "\n",
    "embedding_weights = None\n",
    "\n",
    "with open('./embeddings/swectors-300dim.txt', encoding='utf-8') as file:\n",
    "    # initialize the embedding weights matrix with zeros\n",
    "    dims = [int(x) for x in file.readline().split()]\n",
    "    print(dims)\n",
    "    embedding_weights = torch.zeros((dims[0],dims[1]), dtype=torch.float64)    \n",
    "    \n",
    "    line = file.readline().split()\n",
    "    while line != []:\n",
    "        word2id[line[0]] = id_count\n",
    "        embedding_weights[id_count,:] = torch.tensor([float(x) for x in line[1:]])\n",
    "        id_count += 1\n",
    "        \n",
    "        line = file.readline()\n",
    "        line = line.split()\n",
    "        \n",
    "        if id_count % 100000 == 0:\n",
    "            print(id_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a multilabel_binarizer on the labels\n",
    "labels = df['labels'].tolist()\n",
    "multilab_bin = MultiLabelBinarizer()\n",
    "multilab_bin.fit(labels)\n",
    "\n",
    "# token-ids used for padding and unknown\n",
    "pad_tok = word2id['<>']\n",
    "unk_tok = word2id['<>det']\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, word2id, multilab_bin, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.word2id = word2id\n",
    "        self.multilab_bin = multilab_bin\n",
    "        self.data = dataframe\n",
    "        self.text = self.data['text']\n",
    "        self.labels = self.data['labels']\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        tokens = self.tokenizer(text)\n",
    "        # if tokens is empty, add set tokens to ['x']\n",
    "        if len(tokens) <= 1:\n",
    "            tokens = ['x']\n",
    "        \n",
    "        text_len = len(tokens)\n",
    "        ids = [0 for i in range(len(tokens))]\n",
    "        for i in range(len(tokens)):\n",
    "            if tokens[i] in word2id:\n",
    "                ids[i] = word2id[tokens[i]]\n",
    "            else:\n",
    "                ids[i] = unk_tok\n",
    "        \n",
    "        return {\n",
    "            'lens': text_len,\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'labels': torch.tensor(np.sum(self.multilab_bin.transform([self.labels[index]]), axis=0), dtype=torch.float)\n",
    "        }      \n",
    "    \n",
    "# collate function for the dataloader\n",
    "def collate_fn(batch):\n",
    "    # sort token-id sequences by length\n",
    "    for_sorting = [(batch[i]['lens'],batch[i]['ids'],batch[0]['labels']) for i in range(len(batch))]\n",
    "    for_sorting = sorted(for_sorting, key=lambda tup: tup[0], reverse=True)\n",
    "    \n",
    "    # pad token-id sequences to max_len_batch\n",
    "    max_len_batch = for_sorting[0][0]\n",
    "    \n",
    "    lens = [0 for i in range(len(batch))]\n",
    "    ids = torch.zeros((len(batch), max_len_batch), dtype=torch.long)\n",
    "    labels = torch.zeros((len(batch), len(batch[0]['labels'])))\n",
    "    \n",
    "    for i in range(len(batch)):\n",
    "        temp = torch.tensor([pad_tok for j in range(max_len_batch)])\n",
    "        temp[:for_sorting[i][0]] = for_sorting[i][1]\n",
    "        \n",
    "        lens[i] = for_sorting[i][0]\n",
    "        ids[i,:] = temp\n",
    "        labels[i,:] = for_sorting[i][2]\n",
    "        \n",
    "    return lens, ids, labels\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "Train set: 3027 samples\n",
      "Test set: 757 samples\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# max number of tokens in text\n",
    "#max_len = 200\n",
    "max_len = 0\n",
    "for i in range(len(df['text'])):\n",
    "    text = df['text'][i]\n",
    "            \n",
    "    tokens = basic_tokenization(text)\n",
    "    \n",
    "    if len(tokens) > max_len:\n",
    "        max_len = len(tokens)\n",
    "\n",
    "print(max_len)\n",
    "\n",
    "train_dataset, test_dataset = train_test_split(df,\n",
    "                                        random_state=42,\n",
    "                                        test_size=0.2,\n",
    "                                        shuffle=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "test_dataset = test_dataset.reset_index(drop=True)\n",
    "\n",
    "train_set = CustomDataset(train_dataset, basic_tokenization, word2id, multilab_bin, max_len)\n",
    "test_set = CustomDataset(test_dataset, basic_tokenization, word2id, multilab_bin, max_len)\n",
    "\n",
    "train_params = {'batch_size': batch_size,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0,\n",
    "                'collate_fn': collate_fn\n",
    "               }\n",
    "test_params = {'batch_size': batch_size,\n",
    "               'shuffle': True,\n",
    "               'num_workers': 0,\n",
    "               'collate_fn': collate_fn\n",
    "              }\n",
    "\n",
    "train_loader = DataLoader(train_set, **train_params)\n",
    "test_loader = DataLoader(test_set, **test_params)\n",
    "\n",
    "print(f'Train set: {len(train_dataset)} samples')\n",
    "print(f'Test set: {len(test_dataset)} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = cuda:0\n",
      "Number of available GPUs: 1\n",
      "Device 0: Quadro P520\n"
     ]
    }
   ],
   "source": [
    "# decide which device to use. use cuda if available\n",
    "dev = ''\n",
    "if torch.cuda.is_available():\n",
    "    dev = 'cuda:0'\n",
    "else:\n",
    "    dev = 'cpu'\n",
    "\n",
    "print(f'dev = {dev}')\n",
    "print(f'Number of available GPUs: {torch.cuda.device_count()}')\n",
    "\n",
    "# print the device names\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f'Device {i}: {torch.cuda.get_device_name(i)}')\n",
    "\n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, embeddings, hidden_dim, output_dim):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(embeddings)\n",
    "        self.rnn = nn.LSTM(embeddings.shape[1], hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(2*hidden_dim, output_dim)\n",
    "        self.sigm = nn.Sigmoid()\n",
    "        \n",
    "    # sentences contains padded token-id sequences sorted by length\n",
    "    def forward(self, sentences, sent_lengths):\n",
    "        embeds = self.word_embeddings(sentences)\n",
    "        #print(embeds.shape)\n",
    "        \n",
    "        # initialize h0\n",
    "        #h0 = torch.randn(1*2, sentences.shape[0], sentences.shape[1])\n",
    "        \n",
    "        # pack_padded_sequnce so that padded items won't be shown to the RNN\n",
    "        X = torch.nn.utils.rnn.pack_padded_sequence(embeds, sent_lengths, batch_first=True)\n",
    "        \n",
    "        X = X.float()\n",
    "        _, hc = self.rnn(X)\n",
    "        \n",
    "        # make use of the final hidden state\n",
    "        X = hc[0]\n",
    "        \n",
    "        # undo the packing\n",
    "        #X, _ = torch.nn.utils.rnn.pad_packed_sequence(X, batch_first=True)\n",
    "        \n",
    "        # reshape X to fit the linear layer\n",
    "        #print(X.shape)\n",
    "        #print(self.hidden_dim)\n",
    "        #X = X.contiguous()\n",
    "        X = X.view(-1, 2*self.hidden_dim)\n",
    "        \n",
    "        X = self.fc(X)\n",
    "        X = self.sigm(X)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the loss of an epoch by averaging all batch losses\n",
    "def epoch_loss(model, data_loader, criterion):\n",
    "    loss = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx,batch in enumerate(data_loader):\n",
    "            lens, ids, labels = batch\n",
    "            lens = torch.tensor(lens, dtype=torch.long)#.to(device)\n",
    "            ids = ids.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(ids, lens)\n",
    "            loss += criterion(outputs, labels)\n",
    "            batch_count += 1\n",
    "    model.train()\n",
    "    return loss / batch_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "output_dim = len(multilab_bin.classes_)\n",
    "hidden_dim = 400\n",
    "embedding_weights = embedding_weights.to(device)\n",
    "\n",
    "model = RNN(embedding_weights, hidden_dim, output_dim)\n",
    "model.train()\n",
    "model.to(device)\n",
    "\n",
    "# number of epochs trained\n",
    "epochs_trained = 0\n",
    "\n",
    "# losses over entire train-/test-set per epoch\n",
    "train_losses = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 11, Train Loss: 0.0425072, Test Loss: 0.0424193\n",
      "End of epoch 12, Train Loss: 0.0411836, Test Loss: 0.0433846\n",
      "End of epoch 13, Train Loss: 0.0431265, Test Loss: 0.0419359\n",
      "End of epoch 14, Train Loss: 0.0440535, Test Loss: 0.0502117\n",
      "End of epoch 15, Train Loss: 0.0427039, Test Loss: 0.0383925\n",
      "End of epoch 16, Train Loss: 0.0437093, Test Loss: 0.0408589\n",
      "End of epoch 17, Train Loss: 0.0419721, Test Loss: 0.0405199\n",
      "End of epoch 18, Train Loss: 0.0443111, Test Loss: 0.0481309\n",
      "End of epoch 19, Train Loss: 0.0411908, Test Loss: 0.0367024\n",
      "End of epoch 20, Train Loss: 0.0421413, Test Loss: 0.0417836\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "# training loop\n",
    "learning_rate = 0.00001\n",
    "\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for idx,batch in enumerate(train_loader):\n",
    "        lens, ids, labels = batch\n",
    "        lens = torch.tensor(lens, dtype=torch.long)#.to(device)\n",
    "        ids = ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(ids, lens)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # backward pass\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # save the losses\n",
    "    train_losses.append(epoch_loss(model, train_loader, criterion))\n",
    "    test_losses.append(epoch_loss(model, test_loader, criterion))\n",
    "\n",
    "    epochs_trained += 1\n",
    "    \n",
    "    print(f'End of epoch {epochs_trained}, Train Loss: {train_losses[-1]:.7f}, Test Loss: {test_losses[-1]:.7f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the predicitons and corresponding labels\n",
    "def get_pred_true(model, data_loader, D_out):\n",
    "\n",
    "    y_pred = np.zeros((1,D_out))\n",
    "    y_true = np.zeros((1,D_out))\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        for idx,batch in enumerate(data_loader):\n",
    "            lens, ids, labels = batch\n",
    "            lens = torch.tensor(lens, dtype=torch.long)#.to(device)\n",
    "            ids = ids.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(ids, lens)\n",
    "            \n",
    "            y_pred = np.concatenate((y_pred,outputs.detach().cpu().numpy()), axis=0)\n",
    "            y_true = np.concatenate((y_true,np.array(labels.cpu())), axis=0)\n",
    "            \n",
    "    return y_pred[1:,:], y_true[1:,:]\n",
    "\n",
    "model.eval()  \n",
    "y_pred_temp, y_true = get_pred_true(model, test_loader, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels in training set: 186/197 (94.42 %)\n",
      "Number of labels in test set: 140/197 (71.07 %)\n",
      "Number of labels present in both sets: 129/197 (65.48 %)\n",
      "\n",
      "Micro-average F1-score: 0.28297668476525073\n",
      "Weighted-average F1-score: 0.136232527986836\n",
      "Macro-average F1-score: 0.7872657586337123\n",
      "Accuracy (exact match): 0.13738441215323646\n",
      "Hamming Loss: 0.01505408069523701\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set all values above threshold to 1, else 0\n",
    "th = 0.2\n",
    "y_pred = np.copy(y_pred_temp)\n",
    "y_pred[y_pred > th] = 1\n",
    "y_pred[y_pred <= th] = 0\n",
    "\n",
    "# number of labels in test set\n",
    "#labels_in_test = np.count_nonzero(np.sum(y_true, axis=0))\n",
    "label_counts_test = np.sum(multilab_bin.transform(test_dataset['labels']), axis=0)\n",
    "labels_in_test = np.count_nonzero(label_counts_test)\n",
    "\n",
    "# number of labels in train set\n",
    "label_counts_train = np.sum(multilab_bin.transform(train_dataset['labels']), axis=0)\n",
    "labels_in_train = np.count_nonzero(label_counts_train)\n",
    "\n",
    "# compute size of intersection between labels in train and test\n",
    "label_counts_test[label_counts_test > 0] = 1\n",
    "labels_bin_test = label_counts_test\n",
    "\n",
    "label_counts_train[label_counts_train > 0] = 1\n",
    "labels_bin_train = label_counts_train\n",
    "\n",
    "labels_bin_sum = labels_bin_test + labels_bin_train\n",
    "labels_intersect = np.count_nonzero(labels_bin_sum[labels_bin_sum == 2])\n",
    "\n",
    "print(f'Number of labels in training set: {labels_in_train}/{y_true.shape[1]} ({labels_in_train/y_true.shape[1]*100:.2f} %)')\n",
    "print(f'Number of labels in test set: {labels_in_test}/{y_true.shape[1]} ({labels_in_test/y_true.shape[1]*100:.2f} %)')\n",
    "print(f'Number of labels present in both sets: {labels_intersect}/{y_true.shape[1]} ({labels_intersect/y_true.shape[1]*100:.2f} %)')\n",
    "print()\n",
    "print(f\"Micro-average F1-score: {f1_score(y_true, y_pred, average='micro')}\")\n",
    "print(f\"Weighted-average F1-score: {f1_score(y_true, y_pred, average='weighted', zero_division=1)}\")\n",
    "print(f\"Macro-average F1-score: {f1_score(y_true, y_pred, average='macro', zero_division=1)}\")\n",
    "#print(f\"Sample-average Jaccard score: {jaccard_score(y_true, y_pred, average='samples', zero_division=1)}\")\n",
    "print(f\"Accuracy (exact match): {accuracy_score(y_true, y_pred)}\")\n",
    "print(f\"Hamming Loss: {hamming_loss(y_true, y_pred)}\")\n",
    "\n",
    "x = [x for x in range(1,len(train_losses)+1)]\n",
    "xticks = [x*len(train_losses)//5 for x in range(1,6)]\n",
    "plt.plot(x, test_losses)\n",
    "plt.plot(x, train_losses)\n",
    "plt.legend(['Test Loss', 'Train Loss'])\n",
    "plt.xticks(xticks,xticks)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "#plt.gca().set_ylim([0,0.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.1269e+00, -1.5451e+00,  1.0750e+00, -2.0293e-01, -1.8376e+00,\n",
       "         -8.3120e-02, -1.0068e-01, -2.8981e+00,  1.1431e+00,  4.4147e-01,\n",
       "         -6.3511e-01, -1.1318e+00, -9.2508e-02, -2.3185e+00,  2.0088e+00,\n",
       "          1.6871e-01,  1.3692e+00, -2.2981e+00,  2.1971e+00,  7.3852e-01,\n",
       "         -2.1624e+00,  8.0845e-01, -1.3576e+00,  3.8323e-01,  2.1305e+00,\n",
       "          2.1917e+00, -1.7953e+00,  1.3665e-01,  1.3518e+00, -5.1753e-01,\n",
       "          1.3455e-01,  6.0415e-01, -2.2657e-01, -1.1843e+00,  2.5285e-01,\n",
       "         -1.1356e+00,  4.5385e-01,  4.5484e-02, -5.1116e-01,  1.6789e+00,\n",
       "          7.5456e-01,  2.5622e-01, -2.3226e+00,  1.3944e+00, -9.7285e-01,\n",
       "          1.4166e+00, -2.1429e+00, -1.4608e+00,  1.0985e+00,  1.0250e+00,\n",
       "          6.6112e-01, -4.6495e-01, -4.4374e-01,  1.7232e+00,  3.0483e-02,\n",
       "          1.0165e+00, -2.3558e+00,  2.6003e-01, -2.5648e+00, -2.2209e+00,\n",
       "         -2.5138e+00,  8.5870e-01, -5.1055e-01,  2.0650e+00, -7.4025e-01,\n",
       "         -3.2325e+00,  1.3513e+00,  4.3357e-01, -1.9505e+00, -2.8997e-01,\n",
       "         -5.3353e-01, -1.3721e-01, -2.0809e+00, -1.2394e+00,  8.8540e-01,\n",
       "          2.1496e-01,  9.9882e-01,  4.6393e-01,  8.4486e-01,  9.3359e-01,\n",
       "         -6.3633e-01, -6.1510e-01,  1.3203e+00,  7.5646e-01,  1.5122e+00,\n",
       "          8.9942e-02, -7.8198e-01,  1.6720e+00,  2.1749e+00,  1.8561e+00,\n",
       "          2.8847e+00, -1.4621e+00,  4.7357e-01, -4.1450e-01, -7.9922e-01,\n",
       "          3.9154e-01, -1.1215e+00, -1.1861e+00,  1.2113e+00, -9.4938e-01,\n",
       "         -8.0780e-01, -3.8484e+00, -2.7057e+00, -1.4777e+00, -3.1578e+00,\n",
       "         -5.9508e-01,  1.9455e+00, -6.9525e-01, -4.9522e-01,  2.3031e+00,\n",
       "          2.2601e-01, -6.6940e-01,  2.6374e-02,  4.6226e-01,  7.9200e-03,\n",
       "          5.3547e-01, -1.5617e+00,  1.1321e-02,  1.1967e+00,  2.3883e-01,\n",
       "         -6.1807e-01,  1.3781e-01, -3.0472e-01,  1.2930e+00, -1.9867e-01,\n",
       "         -1.5400e+00, -1.3679e+00, -1.8607e+00,  1.5831e-01, -1.0603e+00,\n",
       "          3.2880e-01, -9.8682e-01,  9.2976e-01,  5.4540e-01, -1.0780e+00,\n",
       "          4.5524e-01, -1.3307e-01,  2.7800e-01, -1.3484e+00, -2.5525e+00,\n",
       "         -5.9978e-01, -2.4961e-01,  1.1910e+00, -1.8516e+00,  1.3271e-01,\n",
       "          1.7007e+00,  1.6935e+00,  1.6986e+00, -2.2343e-01,  2.6132e+00,\n",
       "          3.1574e+00,  3.9185e-01, -3.0691e+00, -9.1631e-01, -7.0838e-01,\n",
       "         -8.4607e-01,  1.3495e-01,  1.3874e+00,  4.2914e-01, -2.0490e+00,\n",
       "         -5.2879e+00,  3.4217e-01,  9.7107e-01, -3.0751e+00, -1.1244e+00,\n",
       "          6.4209e-01, -6.9295e-01,  1.6712e-01,  1.4676e+00, -2.0528e+00,\n",
       "         -3.3552e-01, -9.1245e-01, -5.1486e-01, -1.0591e+00, -4.4547e-01,\n",
       "         -3.4184e+00,  1.5743e+00,  3.6030e-01, -3.0588e+00,  2.4916e+00,\n",
       "         -1.8936e-01, -1.7249e+00,  8.8779e-01, -1.0643e-01,  1.8023e+00,\n",
       "         -7.9782e-02, -1.4134e+00,  2.6322e+00,  1.0461e+00, -1.1145e+00,\n",
       "         -1.7553e+00,  8.5860e-01, -6.1677e-02,  1.8976e+00,  3.4582e-01,\n",
       "          2.8413e-01, -3.0678e-01,  2.0230e+00,  1.4598e+00, -6.6960e-01,\n",
       "          2.9688e-01, -1.2023e+00,  2.6550e+00, -2.2013e+00,  1.8710e-01,\n",
       "         -1.0929e+00, -1.9173e+00,  3.7739e+00,  1.0201e+00, -9.5409e-01,\n",
       "         -2.4924e+00, -3.1042e+00, -5.5266e-01,  4.9806e-01, -9.3543e-02,\n",
       "         -6.7111e-01,  2.3090e-01, -1.4436e+00, -3.2006e-01,  6.2686e-01,\n",
       "          7.9898e-02, -3.3920e+00, -8.7335e-01,  1.7643e-01, -5.1970e-01,\n",
       "         -1.5793e+00,  5.1789e-01, -8.9633e-01, -1.2495e+00,  2.1105e+00,\n",
       "         -8.9552e-01, -4.7240e+00, -6.7051e-01, -7.2961e-01,  1.0438e+00,\n",
       "         -1.0377e-01, -5.5649e-01, -6.2099e-01,  2.1180e+00, -4.5955e-01,\n",
       "          7.4617e-01, -4.0419e-01, -2.1072e+00, -2.3468e+00, -7.2473e-01,\n",
       "         -1.0875e+00, -1.4343e+00,  5.5854e-01,  7.7428e-01, -3.4664e-01,\n",
       "          8.0992e-01,  2.3802e+00,  3.8734e-01,  1.6851e-01, -1.1638e+00,\n",
       "          1.6596e+00, -7.3928e-01, -1.2327e+00, -2.4107e-01,  1.3602e-01,\n",
       "         -3.8095e-01,  1.7330e+00,  1.9443e-01, -2.9707e-01,  2.2606e-01,\n",
       "         -2.8727e-01, -1.5538e-02, -3.1802e-01,  1.8930e+00, -1.0049e+00,\n",
       "          1.1137e+00,  8.9664e-01,  1.0950e+00, -1.3202e+00,  2.2916e+00,\n",
       "          2.0911e+00,  1.0376e+00,  1.5705e+00, -1.2532e+00,  5.5818e-01,\n",
       "         -3.8838e-01,  1.1432e+00,  2.8497e-01, -1.6896e+00, -5.5006e-01,\n",
       "          5.1770e-01, -1.3996e+00,  1.7593e+00, -9.6822e-01, -5.3609e-01,\n",
       "         -2.3121e-01, -8.6878e-01, -1.1485e+00, -2.7632e+00, -2.1300e+00,\n",
       "          1.2123e+00, -1.4259e-01, -1.2130e+00,  4.9033e-01,  1.5822e+00],\n",
       "        [ 4.6350e+00,  8.3475e-01,  9.6622e-01, -5.9226e-01,  1.1853e-01,\n",
       "         -1.9975e+00, -2.2889e+00, -4.2760e+00,  4.6526e-01, -5.3569e-02,\n",
       "         -1.8032e+00, -3.1412e+00,  3.2229e-01, -1.7490e+00,  1.0896e+00,\n",
       "         -1.4396e+00,  5.0752e+00, -3.3113e+00,  3.9011e+00, -7.5814e-01,\n",
       "         -7.0622e-01,  5.0836e+00,  5.2709e-01,  1.1972e+00,  1.8405e+00,\n",
       "          3.1971e-01, -2.4511e+00,  1.6166e-01,  2.7958e+00, -5.8109e-01,\n",
       "          2.4265e+00,  1.2450e+00, -1.4220e+00, -6.7591e+00,  9.2655e-02,\n",
       "         -4.0046e+00, -3.2649e+00, -2.2319e+00,  7.1841e-02,  2.8377e+00,\n",
       "         -8.9718e-01,  2.0126e+00, -3.9547e+00,  2.4021e+00, -3.4475e+00,\n",
       "          4.3618e-01, -1.8587e+00, -2.6888e+00,  2.2725e+00,  4.2694e-01,\n",
       "         -1.1604e+00,  6.7234e-01, -4.7883e+00,  5.6181e+00,  4.2491e+00,\n",
       "          4.7154e+00,  2.1425e+00,  2.9997e+00, -1.1123e+00, -2.5159e-01,\n",
       "         -1.0382e-01,  7.5886e-02, -6.9007e-01,  3.5494e+00, -3.2808e+00,\n",
       "         -3.7387e+00,  2.1934e+00,  4.5047e+00, -4.7134e+00, -8.4075e-01,\n",
       "          4.5307e+00,  7.1012e-02, -2.6707e+00, -3.7232e-01,  1.8029e+00,\n",
       "          2.4782e+00, -2.7549e+00, -3.9267e+00,  3.3040e-01, -2.0142e+00,\n",
       "          7.7793e-01,  3.7416e-01,  2.6070e-01,  4.9050e+00, -2.0884e+00,\n",
       "          4.0735e+00, -3.3262e+00,  2.9540e+00, -1.8905e+00,  3.0229e+00,\n",
       "         -2.5120e+00, -1.2213e-01,  1.0909e+00,  3.4619e+00, -8.9878e-01,\n",
       "          1.6230e+00,  9.9011e-01, -6.1630e+00,  2.8538e+00, -2.3430e+00,\n",
       "          1.5630e-01, -4.4553e+00, -3.4419e+00,  1.5007e+00, -8.2960e-01,\n",
       "          1.6438e+00,  5.9930e+00,  2.1050e+00, -5.0064e-01,  1.0386e+00,\n",
       "         -3.1702e+00,  2.3835e+00,  1.0199e+00, -3.0759e+00, -1.8656e+00,\n",
       "         -1.9698e+00,  2.2565e+00,  3.7905e+00,  5.6548e+00,  7.3826e-01,\n",
       "         -1.3013e+00,  2.2771e-02,  2.7741e+00, -3.5648e-02, -4.6323e-01,\n",
       "         -3.7018e+00,  2.3487e-01, -1.7133e+00,  4.8635e+00, -5.0447e+00,\n",
       "          9.3903e-01, -1.5495e+00,  1.8522e+00,  3.7441e+00, -4.8567e+00,\n",
       "         -2.1816e+00,  1.0572e-01, -2.9075e-01,  1.3832e+00,  5.3346e-01,\n",
       "         -1.2625e-01,  1.5178e+00, -1.3800e+00, -4.3871e+00,  2.8265e-02,\n",
       "          2.3708e-01,  1.0093e+00,  3.6357e+00,  2.2280e+00,  4.1311e+00,\n",
       "          3.3171e+00,  2.1308e+00,  2.2853e+00, -9.1019e-01,  3.0817e+00,\n",
       "          1.4046e+00, -6.9348e-01, -1.4414e+00, -1.6972e+00, -8.0396e+00,\n",
       "         -4.4592e+00,  3.6472e+00, -3.0605e-01, -6.8100e+00, -2.6059e+00,\n",
       "          1.0990e-01, -3.9150e+00, -2.8750e+00, -8.3010e-01, -4.2588e+00,\n",
       "         -3.6208e+00,  2.5559e+00,  1.0223e+00, -1.0466e+00, -1.3660e+00,\n",
       "         -5.7685e-02,  2.0365e+00, -3.3822e-01, -1.7457e+00,  1.4028e+00,\n",
       "         -6.0682e+00,  8.1843e-01,  4.1493e+00, -6.2116e+00,  5.5315e-01,\n",
       "         -9.6227e-02,  2.0300e+00,  2.1431e+00, -2.1262e-01,  3.5091e-01,\n",
       "          1.8991e+00, -1.7490e+00, -1.2607e+00,  3.9112e+00,  1.1119e+00,\n",
       "          2.0501e+00, -1.2614e+00,  3.1303e+00,  2.0012e+00,  1.1754e+00,\n",
       "         -2.7074e+00,  2.1868e-01,  7.3630e+00, -8.5594e-01,  6.2754e-01,\n",
       "         -3.8161e+00,  7.8657e-02,  7.1392e+00, -2.2517e+00,  1.5407e+00,\n",
       "         -1.2258e+00, -7.7707e-01, -1.8734e+00, -2.6163e+00, -3.0306e-01,\n",
       "          7.3575e-01, -3.6494e-01, -1.7583e-01, -2.7486e+00, -2.7031e+00,\n",
       "         -2.2115e+00, -2.5293e+00, -3.2832e+00,  1.4909e+00,  1.2311e+00,\n",
       "         -4.1808e-01,  2.6686e+00,  4.4712e+00, -1.4085e+00, -5.7639e-01,\n",
       "          6.4366e-01, -2.0412e+00,  9.0324e-01, -3.5762e+00,  3.3575e+00,\n",
       "         -4.4250e+00,  1.2847e+00, -4.2691e-01, -6.5234e-01,  9.2608e-01,\n",
       "          4.1036e+00, -1.3382e+00,  2.2599e+00,  2.8477e+00,  6.4565e-01,\n",
       "          1.5500e+00, -3.3105e+00,  3.0503e+00,  3.4023e+00,  3.3112e+00,\n",
       "         -1.3763e-01,  3.6314e+00,  6.1908e-01, -1.4647e-01, -6.7645e+00,\n",
       "         -2.3252e+00,  5.0870e-01,  2.6738e+00, -2.2415e+00, -3.8953e+00,\n",
       "         -1.9286e-01,  1.6902e+00, -3.2933e+00,  7.2735e-01,  1.8010e+00,\n",
       "         -2.3360e-01, -3.0758e+00, -4.6786e-01,  1.3750e+00,  1.5074e+00,\n",
       "          4.9045e-01,  1.3188e+00, -7.0635e-01,  3.8881e-01, -3.6402e-01,\n",
       "          3.4860e+00,  9.0681e-01, -1.6304e+00, -1.3793e+00,  1.7574e+00,\n",
       "         -5.6552e-01,  2.5379e+00, -7.6642e-01, -5.7518e+00,  1.0674e+00,\n",
       "          3.3470e-01, -3.3722e+00,  3.3228e-01, -2.6594e+00,  2.2237e+00,\n",
       "         -5.2803e-01, -3.7322e-01, -1.8875e+00, -3.1385e+00, -2.2572e+00,\n",
       "          2.3949e+00, -2.6959e+00, -4.5444e+00, -3.1763e+00, -1.2721e+00],\n",
       "        [-5.0352e-01,  1.7010e+00,  2.1274e+00, -3.2288e+00, -1.6174e+00,\n",
       "         -3.5484e+00,  2.0741e-01, -1.1144e+00, -8.1229e-02, -1.0137e+00,\n",
       "         -3.8565e+00, -1.9628e+00,  1.0643e+00, -3.1227e+00,  1.0755e+00,\n",
       "         -3.7167e-01,  5.4218e+00, -3.9471e+00,  1.5422e-01,  4.9153e-01,\n",
       "         -3.9574e+00,  6.1355e-01, -1.8347e+00, -4.6962e+00,  1.9497e+00,\n",
       "         -1.2012e+00, -3.9428e-02,  2.5411e+00, -1.2898e+00,  3.5466e+00,\n",
       "         -1.8249e-01,  3.8338e+00, -1.1254e-01, -8.0559e-01,  2.4772e+00,\n",
       "         -4.4549e+00, -3.3774e+00, -1.2661e+00,  1.4536e-01,  3.5552e-01,\n",
       "         -3.2861e+00, -1.1897e+00, -3.0400e+00, -2.2222e-01, -2.8903e+00,\n",
       "          2.9399e+00, -1.3130e+00, -5.6712e-01,  4.0514e-01, -4.8199e-01,\n",
       "          3.7696e+00, -5.3175e-01, -1.7187e+00,  5.4244e-01,  2.1240e+00,\n",
       "          2.9934e+00, -2.3636e+00,  3.3772e+00, -1.3248e+00, -2.4433e+00,\n",
       "          9.4054e-01,  5.6442e+00, -1.8508e+00,  9.0796e-01, -1.1372e+00,\n",
       "         -5.5279e+00,  8.5346e-01, -8.8913e-01, -8.2176e-01,  7.0204e-01,\n",
       "          3.7425e+00, -9.4648e-01, -1.1348e+00,  2.0783e+00,  6.1466e-01,\n",
       "          3.6501e+00,  1.2264e+00,  1.2228e+00,  2.5931e+00, -2.9749e+00,\n",
       "          6.0770e-01,  3.9407e+00,  2.9956e+00,  2.8804e+00,  3.0828e-01,\n",
       "          2.8066e+00,  2.8206e-01,  3.3304e+00,  5.8135e-01,  2.0367e+00,\n",
       "         -5.5730e-01,  2.0078e+00,  1.8821e+00,  5.9857e-01,  1.4796e+00,\n",
       "         -1.3977e+00, -6.5613e-02, -2.3920e+00,  1.3821e+00, -8.1158e-01,\n",
       "         -3.5310e+00, -3.6863e+00,  2.0069e+00, -1.4545e+00,  1.8439e-01,\n",
       "          7.2825e-01,  4.4314e+00, -9.3339e-02,  8.1414e-01, -1.0621e+00,\n",
       "         -2.1906e+00,  6.4241e-01,  1.2510e+00,  2.8496e-01,  3.9252e+00,\n",
       "          1.0372e+00, -1.4323e+00, -1.2324e+00,  3.3195e+00, -1.2634e+00,\n",
       "         -1.8774e+00,  2.8111e+00,  8.9639e-01, -9.1787e-01,  3.2884e+00,\n",
       "         -5.4466e+00,  8.9962e-01,  6.1619e-01,  1.6977e+00, -1.1975e+00,\n",
       "         -2.2728e-01, -9.8915e-01,  4.2526e-01,  2.0172e-01, -5.7621e+00,\n",
       "         -2.4242e-01,  6.0183e-01,  2.8416e-01, -4.1238e-01,  1.1126e-02,\n",
       "         -9.6510e-03, -2.8741e+00,  1.5240e+00, -1.1525e+00, -5.3347e-01,\n",
       "          9.8574e-01,  1.1964e+00,  2.8305e-01,  6.2846e-01,  2.4960e+00,\n",
       "          2.9279e+00, -7.5770e-01, -1.9079e+00,  5.6015e-01,  1.6236e+00,\n",
       "          7.4475e-01, -7.5493e-01, -1.2471e+00,  3.3544e-01, -1.5805e+00,\n",
       "         -4.4680e+00,  9.2944e-01,  5.0142e+00, -5.7380e+00, -5.9360e-01,\n",
       "          2.6646e+00,  2.3972e+00, -2.0446e+00, -4.4062e+00, -1.9926e+00,\n",
       "          8.1280e-03,  7.3684e-01,  1.0392e+00,  2.6199e+00,  2.6070e+00,\n",
       "          3.2985e+00,  1.5195e-01,  1.1442e+00, -2.8051e-02,  1.6765e+00,\n",
       "         -2.9313e-01, -1.3516e-01,  2.5003e+00, -3.3510e+00, -1.3679e+00,\n",
       "         -1.8452e+00, -9.4895e-02, -9.5268e-01,  1.1292e+00,  2.3195e+00,\n",
       "         -2.6583e-02, -1.9926e+00, -2.5707e+00,  1.3888e+00,  1.2445e-01,\n",
       "         -3.7403e-01,  1.6183e-01, -1.1343e+00,  2.1649e+00,  2.9848e-02,\n",
       "         -1.1664e-01,  5.4562e-01,  3.8438e+00,  1.1434e+00,  1.2165e+00,\n",
       "         -3.8711e+00,  1.2023e+00,  8.9138e-01, -3.8142e-01, -6.8504e-01,\n",
       "          9.8821e-01, -6.4634e-01, -9.6959e-01, -1.1275e+00, -2.0442e+00,\n",
       "         -1.6918e+00,  2.3971e+00,  1.2962e+00, -2.8102e+00,  1.9395e+00,\n",
       "          9.7418e-01, -1.0018e+00, -5.6671e-01,  2.0925e+00, -1.9863e+00,\n",
       "          1.6535e+00,  4.0171e+00,  2.8145e+00, -3.5206e+00, -2.9452e-01,\n",
       "         -1.9579e-02, -2.9527e+00,  1.8286e-02, -4.1769e+00,  2.4870e+00,\n",
       "         -5.2746e-01,  9.1440e-01, -3.4508e+00,  1.1058e+00, -1.3841e+00,\n",
       "          3.6122e+00, -3.3112e+00, -2.6342e-02,  9.2765e-01, -1.0404e+00,\n",
       "          1.3818e+00, -3.0424e+00, -5.0374e-01,  1.5759e+00,  2.1898e+00,\n",
       "         -1.6565e+00,  9.5690e-01, -1.4051e+00,  3.4217e+00, -4.1971e+00,\n",
       "         -1.2622e+00, -4.7810e-01,  7.9199e-01, -7.9441e-01, -2.6026e+00,\n",
       "          1.3699e+00,  7.4175e-01,  3.2836e+00,  1.4616e+00, -1.6954e+00,\n",
       "          1.0302e+00,  1.6578e+00,  5.6962e-01,  4.6615e-01,  1.0882e+00,\n",
       "          2.8064e+00,  1.6886e+00, -1.7401e+00, -7.8728e-01,  3.7709e+00,\n",
       "          2.2766e+00,  3.2204e+00, -2.3539e+00,  1.1288e+00,  4.6021e-01,\n",
       "          2.1343e+00,  1.3949e+00, -3.0235e+00,  2.2167e-01, -2.7422e+00,\n",
       "         -4.0913e-01, -2.3828e+00, -7.0550e-01, -3.8964e+00,  2.0292e+00,\n",
       "          1.5543e+00, -2.0528e-01,  2.1525e+00,  1.0962e+00,  1.1850e+00,\n",
       "          6.5744e-01, -7.0248e-01, -1.6439e+00, -8.7276e-01,  6.5251e-02]],\n",
       "       device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings = nn.Embedding.from_pretrained(embedding_weights)\n",
    "\n",
    "word_embeddings(torch.tensor([1,2,3], dtype=torch.long).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
