{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ReHiq1Llr_7M",
    "outputId": "ca41a022-60a8-40b6-91c5-d6c5b59a7d67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AqMdrxYy2iin",
    "outputId": "b24f1b85-aa6f-4196-f99d-34d41b1f03e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/exjobb_project\n",
      "/content/drive/MyDrive/exjobb_project\n",
      "['bert-base-swedish-cased', 'electra-small-swedish-cased-discriminator', 'electra-base-swedish-cased-discriminator']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if os.getcwd() == '/content':\n",
    "  % cd drive/MyDrive/exjobb_project\n",
    "\n",
    "print(os.getcwd())\n",
    "print(os.listdir('bert'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ztQ8dAWOmau7"
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer\n",
    "from transformers import PretrainedConfig\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import BCELoss\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LuzjiHVMmau9"
   },
   "outputs": [],
   "source": [
    "path_to_data = './data'\n",
    "\n",
    "# create dataframe from sessions.json\n",
    "df = pd.read_json(f'{path_to_data}/sessions.json')\n",
    "df.head()\n",
    "\n",
    "# create dictionaries for switching between symptom and id\n",
    "id2sym = {}\n",
    "sym2id = {}\n",
    "\n",
    "with open(f'{path_to_data}/symptoms.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    for sym in data:\n",
    "        id2sym[sym['id']] = sym['name']\n",
    "        sym2id[sym['name']] = sym['id']\n",
    "        \n",
    "        \n",
    "# remove labels that have less than m occurrences\n",
    "m = 0\n",
    "\n",
    "labels_list = df['confirmed'].tolist()\n",
    "labels_list = sum(labels_list, [])\n",
    "c = Counter(labels_list)\n",
    "for i in range(len(df)):\n",
    "    to_remove = []\n",
    "    \n",
    "    # find labels that should be removed \n",
    "    for j in range(len(df['confirmed'][i])):\n",
    "        if c[df['confirmed'][i][j]] < m:\n",
    "            to_remove.append(j)\n",
    "            \n",
    "    # remove the labels\n",
    "    shift = 0\n",
    "    for j in range(len(to_remove)):\n",
    "        df['confirmed'][i].pop(to_remove[j]-shift)\n",
    "        shift += 1\n",
    "    \n",
    "        \n",
    "# add column with the symptom names\n",
    "sym_names = []\n",
    "\n",
    "for syms in df['confirmed']:\n",
    "    if len(syms) != 0:\n",
    "        sym_names.append([id2sym[x] for x in syms])\n",
    "    else:\n",
    "        sym_names.append([])\n",
    "\n",
    "df['labels'] = sym_names\n",
    "\n",
    "# remove all rows with no confirmed labels\n",
    "df = df[df['confirmed'].map(len) > 0]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "m9xph3WAmau-",
    "outputId": "b6327551-8a53-4785-d6ff-149165fd19ca"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Slut på medicin.</td>\n",
       "      <td>[Känd astma, Känd lungsjukdom]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Behöver att prata med psykolog angående använd...</td>\n",
       "      <td>[Nedstämdhet, Trötthet]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Har fått besvärlig eksem på händerna</td>\n",
       "      <td>[Hudbesvär, Synliga hudbesvär]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Muskelsvaghet och trötthet känner mig skakig o...</td>\n",
       "      <td>[Muskelsvaghet, Trötthet]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Svår smärta i vänsterhanden/handleden precis n...</td>\n",
       "      <td>[Smärta i handled eller fingrar, Förvärras av ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text                                             labels\n",
       "0                                   Slut på medicin.                     [Känd astma, Känd lungsjukdom]\n",
       "1  Behöver att prata med psykolog angående använd...                            [Nedstämdhet, Trötthet]\n",
       "2              Har fått besvärlig eksem på händerna                      [Hudbesvär, Synliga hudbesvär]\n",
       "3  Muskelsvaghet och trötthet känner mig skakig o...                          [Muskelsvaghet, Trötthet]\n",
       "4  Svår smärta i vänsterhanden/handleden precis n...  [Smärta i handled eller fingrar, Förvärras av ..."
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop('confirmed', inplace=True, axis=1)\n",
    "df.drop('suggested', inplace=True, axis=1)\n",
    "#df = df[0:100]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Mp_L0-yhmau_"
   },
   "outputs": [],
   "source": [
    "# choose which transformer model to use\n",
    "path_to_bert = r'./bert/electra-small-swedish-cased-discriminator'\n",
    "#path_to_bert = r'./bert/electra-base-swedish-cased-discriminator'\n",
    "#path_to_bert = r'./bert/bert-base-swedish-cased'\n",
    "\n",
    "tok = BertTokenizer.from_pretrained(path_to_bert)\n",
    "\n",
    "# train a multilabel_binarizer on the labels\n",
    "labels = df['labels'].tolist()\n",
    "multilab_bin = MultiLabelBinarizer()\n",
    "multilab_bin.fit(labels)\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, multilab_bin, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.multilab_bin = multilab_bin\n",
    "        self.data = dataframe\n",
    "        self.text = self.data['text']\n",
    "        self.labels = self.data['labels']\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs['token_type_ids']\n",
    "        \n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            #'labels': torch.tensor(np.sum(self.multilab_bin.transform([self.labels[index]]), axis=0), dtype=torch.float)\n",
    "            'labels': torch.tensor(self.multilab_bin.transform([self.labels[index]]).reshape(-1,), dtype=torch.float)\n",
    "        }        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vqxfp2B8mau_",
    "outputId": "55be4747-690a-47ae-badf-3702e3549148"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max token length: 174\n",
      "Train set: 3027 samples\n",
      "Test set: 757 samples\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# max number of tokens in text\n",
    "#max_len = 200\n",
    "max_len = 0\n",
    "for i in range(len(df['text'])):\n",
    "  text = df['text'][i]\n",
    "  inputs = tok.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            padding=False,\n",
    "            return_token_type_ids=True\n",
    "            )\n",
    "  if len(inputs['input_ids']) > max_len:\n",
    "    max_len = len(inputs['input_ids'])\n",
    "print(f'Max token length: {max_len}')\n",
    "\n",
    "\n",
    "train_dataset, test_dataset = train_test_split(df,\n",
    "                                        random_state=42,\n",
    "                                        test_size=0.2,\n",
    "                                        shuffle=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "test_dataset = test_dataset.reset_index(drop=True)\n",
    "\n",
    "train_set = CustomDataset(train_dataset, tok, multilab_bin, max_len)\n",
    "test_set = CustomDataset(test_dataset, tok, multilab_bin, max_len)\n",
    "\n",
    "train_params = {'batch_size': batch_size,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "               }\n",
    "test_params = {'batch_size': batch_size,\n",
    "               'shuffle': True,\n",
    "               'num_workers': 0\n",
    "              }\n",
    "\n",
    "train_loader = DataLoader(train_set, **train_params)\n",
    "test_loader = DataLoader(test_set, **test_params)\n",
    "\n",
    "print(f'Train set: {len(train_dataset)} samples')\n",
    "print(f'Test set: {len(test_dataset)} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vHjybrQHmau_",
    "outputId": "56142d18-d14b-4d0d-d8dc-9aeb8fa45389"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = cuda:0\n",
      "Number of available GPUs: 1\n",
      "Device 0: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# decide which device to use. use cuda if available\n",
    "dev = ''\n",
    "if torch.cuda.is_available():\n",
    "    dev = 'cuda:0'\n",
    "else:\n",
    "    dev = 'cpu'\n",
    "\n",
    "print(f'dev = {dev}')\n",
    "print(f'Number of available GPUs: {torch.cuda.device_count()}')\n",
    "\n",
    "# print the device names\n",
    "for i in range(torch.cuda.device_count()):\n",
    "  print(f'Device {i}: {torch.cuda.get_device_name(i)}')\n",
    "\n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "lBYDTZNvmavA"
   },
   "outputs": [],
   "source": [
    "class BERTClass(nn.Module):\n",
    "    def __init__(self, config, path_to_bert, output_dim):\n",
    "        super(BERTClass, self).__init__()\n",
    "        #config = PretrainedConfig.from_json_file(f'{path_to_bert}/config.json')\n",
    "        self.bert = BertModel.from_pretrained(path_to_bert)\n",
    "        #self.bert = BertModel(config)\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.fc = nn.Linear(config.hidden_size, output_dim)\n",
    "        self.sigm = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        x = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        x = self.dropout(x[1])\n",
    "        x = self.fc(x)\n",
    "        output = self.sigm(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "XGGhVr_tmavA"
   },
   "outputs": [],
   "source": [
    "# compute the loss of an epoch by averaging all batch losses\n",
    "def epoch_loss(model, data_loader, criterion):\n",
    "    loss = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx,batch in enumerate(data_loader):\n",
    "            ids = batch['ids'].to(device, dtype=torch.long)\n",
    "            mask = batch['mask'].to(device, dtype=torch.long)\n",
    "            token_type_ids = batch['token_type_ids'].to(device, dtype=torch.long)\n",
    "            labels = batch['labels'].to(device, dtype=torch.float)\n",
    "            \n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            loss += criterion(outputs, labels)\n",
    "            batch_count += 1\n",
    "    model.train()\n",
    "    return loss / batch_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gr7rj2zwWj0K",
    "outputId": "f1324835-311c-4b9d-b678-075f1c40f2f6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert/electra-small-swedish-cased-discriminator were not used when initializing BertModel: ['electra.embeddings.position_ids', 'electra.embeddings.word_embeddings.weight', 'electra.embeddings.position_embeddings.weight', 'electra.embeddings.token_type_embeddings.weight', 'electra.embeddings.LayerNorm.weight', 'electra.embeddings.LayerNorm.bias', 'electra.embeddings_project.weight', 'electra.embeddings_project.bias', 'electra.encoder.layer.0.attention.self.query.weight', 'electra.encoder.layer.0.attention.self.query.bias', 'electra.encoder.layer.0.attention.self.key.weight', 'electra.encoder.layer.0.attention.self.key.bias', 'electra.encoder.layer.0.attention.self.value.weight', 'electra.encoder.layer.0.attention.self.value.bias', 'electra.encoder.layer.0.attention.output.dense.weight', 'electra.encoder.layer.0.attention.output.dense.bias', 'electra.encoder.layer.0.attention.output.LayerNorm.weight', 'electra.encoder.layer.0.attention.output.LayerNorm.bias', 'electra.encoder.layer.0.intermediate.dense.weight', 'electra.encoder.layer.0.intermediate.dense.bias', 'electra.encoder.layer.0.output.dense.weight', 'electra.encoder.layer.0.output.dense.bias', 'electra.encoder.layer.0.output.LayerNorm.weight', 'electra.encoder.layer.0.output.LayerNorm.bias', 'electra.encoder.layer.1.attention.self.query.weight', 'electra.encoder.layer.1.attention.self.query.bias', 'electra.encoder.layer.1.attention.self.key.weight', 'electra.encoder.layer.1.attention.self.key.bias', 'electra.encoder.layer.1.attention.self.value.weight', 'electra.encoder.layer.1.attention.self.value.bias', 'electra.encoder.layer.1.attention.output.dense.weight', 'electra.encoder.layer.1.attention.output.dense.bias', 'electra.encoder.layer.1.attention.output.LayerNorm.weight', 'electra.encoder.layer.1.attention.output.LayerNorm.bias', 'electra.encoder.layer.1.intermediate.dense.weight', 'electra.encoder.layer.1.intermediate.dense.bias', 'electra.encoder.layer.1.output.dense.weight', 'electra.encoder.layer.1.output.dense.bias', 'electra.encoder.layer.1.output.LayerNorm.weight', 'electra.encoder.layer.1.output.LayerNorm.bias', 'electra.encoder.layer.2.attention.self.query.weight', 'electra.encoder.layer.2.attention.self.query.bias', 'electra.encoder.layer.2.attention.self.key.weight', 'electra.encoder.layer.2.attention.self.key.bias', 'electra.encoder.layer.2.attention.self.value.weight', 'electra.encoder.layer.2.attention.self.value.bias', 'electra.encoder.layer.2.attention.output.dense.weight', 'electra.encoder.layer.2.attention.output.dense.bias', 'electra.encoder.layer.2.attention.output.LayerNorm.weight', 'electra.encoder.layer.2.attention.output.LayerNorm.bias', 'electra.encoder.layer.2.intermediate.dense.weight', 'electra.encoder.layer.2.intermediate.dense.bias', 'electra.encoder.layer.2.output.dense.weight', 'electra.encoder.layer.2.output.dense.bias', 'electra.encoder.layer.2.output.LayerNorm.weight', 'electra.encoder.layer.2.output.LayerNorm.bias', 'electra.encoder.layer.3.attention.self.query.weight', 'electra.encoder.layer.3.attention.self.query.bias', 'electra.encoder.layer.3.attention.self.key.weight', 'electra.encoder.layer.3.attention.self.key.bias', 'electra.encoder.layer.3.attention.self.value.weight', 'electra.encoder.layer.3.attention.self.value.bias', 'electra.encoder.layer.3.attention.output.dense.weight', 'electra.encoder.layer.3.attention.output.dense.bias', 'electra.encoder.layer.3.attention.output.LayerNorm.weight', 'electra.encoder.layer.3.attention.output.LayerNorm.bias', 'electra.encoder.layer.3.intermediate.dense.weight', 'electra.encoder.layer.3.intermediate.dense.bias', 'electra.encoder.layer.3.output.dense.weight', 'electra.encoder.layer.3.output.dense.bias', 'electra.encoder.layer.3.output.LayerNorm.weight', 'electra.encoder.layer.3.output.LayerNorm.bias', 'electra.encoder.layer.4.attention.self.query.weight', 'electra.encoder.layer.4.attention.self.query.bias', 'electra.encoder.layer.4.attention.self.key.weight', 'electra.encoder.layer.4.attention.self.key.bias', 'electra.encoder.layer.4.attention.self.value.weight', 'electra.encoder.layer.4.attention.self.value.bias', 'electra.encoder.layer.4.attention.output.dense.weight', 'electra.encoder.layer.4.attention.output.dense.bias', 'electra.encoder.layer.4.attention.output.LayerNorm.weight', 'electra.encoder.layer.4.attention.output.LayerNorm.bias', 'electra.encoder.layer.4.intermediate.dense.weight', 'electra.encoder.layer.4.intermediate.dense.bias', 'electra.encoder.layer.4.output.dense.weight', 'electra.encoder.layer.4.output.dense.bias', 'electra.encoder.layer.4.output.LayerNorm.weight', 'electra.encoder.layer.4.output.LayerNorm.bias', 'electra.encoder.layer.5.attention.self.query.weight', 'electra.encoder.layer.5.attention.self.query.bias', 'electra.encoder.layer.5.attention.self.key.weight', 'electra.encoder.layer.5.attention.self.key.bias', 'electra.encoder.layer.5.attention.self.value.weight', 'electra.encoder.layer.5.attention.self.value.bias', 'electra.encoder.layer.5.attention.output.dense.weight', 'electra.encoder.layer.5.attention.output.dense.bias', 'electra.encoder.layer.5.attention.output.LayerNorm.weight', 'electra.encoder.layer.5.attention.output.LayerNorm.bias', 'electra.encoder.layer.5.intermediate.dense.weight', 'electra.encoder.layer.5.intermediate.dense.bias', 'electra.encoder.layer.5.output.dense.weight', 'electra.encoder.layer.5.output.dense.bias', 'electra.encoder.layer.5.output.LayerNorm.weight', 'electra.encoder.layer.5.output.LayerNorm.bias', 'electra.encoder.layer.6.attention.self.query.weight', 'electra.encoder.layer.6.attention.self.query.bias', 'electra.encoder.layer.6.attention.self.key.weight', 'electra.encoder.layer.6.attention.self.key.bias', 'electra.encoder.layer.6.attention.self.value.weight', 'electra.encoder.layer.6.attention.self.value.bias', 'electra.encoder.layer.6.attention.output.dense.weight', 'electra.encoder.layer.6.attention.output.dense.bias', 'electra.encoder.layer.6.attention.output.LayerNorm.weight', 'electra.encoder.layer.6.attention.output.LayerNorm.bias', 'electra.encoder.layer.6.intermediate.dense.weight', 'electra.encoder.layer.6.intermediate.dense.bias', 'electra.encoder.layer.6.output.dense.weight', 'electra.encoder.layer.6.output.dense.bias', 'electra.encoder.layer.6.output.LayerNorm.weight', 'electra.encoder.layer.6.output.LayerNorm.bias', 'electra.encoder.layer.7.attention.self.query.weight', 'electra.encoder.layer.7.attention.self.query.bias', 'electra.encoder.layer.7.attention.self.key.weight', 'electra.encoder.layer.7.attention.self.key.bias', 'electra.encoder.layer.7.attention.self.value.weight', 'electra.encoder.layer.7.attention.self.value.bias', 'electra.encoder.layer.7.attention.output.dense.weight', 'electra.encoder.layer.7.attention.output.dense.bias', 'electra.encoder.layer.7.attention.output.LayerNorm.weight', 'electra.encoder.layer.7.attention.output.LayerNorm.bias', 'electra.encoder.layer.7.intermediate.dense.weight', 'electra.encoder.layer.7.intermediate.dense.bias', 'electra.encoder.layer.7.output.dense.weight', 'electra.encoder.layer.7.output.dense.bias', 'electra.encoder.layer.7.output.LayerNorm.weight', 'electra.encoder.layer.7.output.LayerNorm.bias', 'electra.encoder.layer.8.attention.self.query.weight', 'electra.encoder.layer.8.attention.self.query.bias', 'electra.encoder.layer.8.attention.self.key.weight', 'electra.encoder.layer.8.attention.self.key.bias', 'electra.encoder.layer.8.attention.self.value.weight', 'electra.encoder.layer.8.attention.self.value.bias', 'electra.encoder.layer.8.attention.output.dense.weight', 'electra.encoder.layer.8.attention.output.dense.bias', 'electra.encoder.layer.8.attention.output.LayerNorm.weight', 'electra.encoder.layer.8.attention.output.LayerNorm.bias', 'electra.encoder.layer.8.intermediate.dense.weight', 'electra.encoder.layer.8.intermediate.dense.bias', 'electra.encoder.layer.8.output.dense.weight', 'electra.encoder.layer.8.output.dense.bias', 'electra.encoder.layer.8.output.LayerNorm.weight', 'electra.encoder.layer.8.output.LayerNorm.bias', 'electra.encoder.layer.9.attention.self.query.weight', 'electra.encoder.layer.9.attention.self.query.bias', 'electra.encoder.layer.9.attention.self.key.weight', 'electra.encoder.layer.9.attention.self.key.bias', 'electra.encoder.layer.9.attention.self.value.weight', 'electra.encoder.layer.9.attention.self.value.bias', 'electra.encoder.layer.9.attention.output.dense.weight', 'electra.encoder.layer.9.attention.output.dense.bias', 'electra.encoder.layer.9.attention.output.LayerNorm.weight', 'electra.encoder.layer.9.attention.output.LayerNorm.bias', 'electra.encoder.layer.9.intermediate.dense.weight', 'electra.encoder.layer.9.intermediate.dense.bias', 'electra.encoder.layer.9.output.dense.weight', 'electra.encoder.layer.9.output.dense.bias', 'electra.encoder.layer.9.output.LayerNorm.weight', 'electra.encoder.layer.9.output.LayerNorm.bias', 'electra.encoder.layer.10.attention.self.query.weight', 'electra.encoder.layer.10.attention.self.query.bias', 'electra.encoder.layer.10.attention.self.key.weight', 'electra.encoder.layer.10.attention.self.key.bias', 'electra.encoder.layer.10.attention.self.value.weight', 'electra.encoder.layer.10.attention.self.value.bias', 'electra.encoder.layer.10.attention.output.dense.weight', 'electra.encoder.layer.10.attention.output.dense.bias', 'electra.encoder.layer.10.attention.output.LayerNorm.weight', 'electra.encoder.layer.10.attention.output.LayerNorm.bias', 'electra.encoder.layer.10.intermediate.dense.weight', 'electra.encoder.layer.10.intermediate.dense.bias', 'electra.encoder.layer.10.output.dense.weight', 'electra.encoder.layer.10.output.dense.bias', 'electra.encoder.layer.10.output.LayerNorm.weight', 'electra.encoder.layer.10.output.LayerNorm.bias', 'electra.encoder.layer.11.attention.self.query.weight', 'electra.encoder.layer.11.attention.self.query.bias', 'electra.encoder.layer.11.attention.self.key.weight', 'electra.encoder.layer.11.attention.self.key.bias', 'electra.encoder.layer.11.attention.self.value.weight', 'electra.encoder.layer.11.attention.self.value.bias', 'electra.encoder.layer.11.attention.output.dense.weight', 'electra.encoder.layer.11.attention.output.dense.bias', 'electra.encoder.layer.11.attention.output.LayerNorm.weight', 'electra.encoder.layer.11.attention.output.LayerNorm.bias', 'electra.encoder.layer.11.intermediate.dense.weight', 'electra.encoder.layer.11.intermediate.dense.bias', 'electra.encoder.layer.11.output.dense.weight', 'electra.encoder.layer.11.output.dense.bias', 'electra.encoder.layer.11.output.LayerNorm.weight', 'electra.encoder.layer.11.output.LayerNorm.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ./bert/electra-small-swedish-cased-discriminator and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "D_out = len(multilab_bin.classes_)\n",
    "\n",
    "config = PretrainedConfig.from_json_file(f'{path_to_bert}/config.json')\n",
    "\n",
    "model = BERTClass(config, path_to_bert, D_out)\n",
    "model.train()\n",
    "model.to(device)\n",
    "\n",
    "# number of epochs trained\n",
    "epochs_trained = 0\n",
    "\n",
    "# losses over entire train-/test-set per epoch\n",
    "train_losses = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bs7jzf1amavA",
    "outputId": "b103fd69-8453-4cd1-ede4-c82deeef474e",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 61, Train Loss: 0.0113513, Test Loss: 0.0246039\n",
      "End of epoch 62, Train Loss: 0.0111072, Test Loss: 0.0248307\n",
      "End of epoch 63, Train Loss: 0.0109023, Test Loss: 0.0245282\n",
      "End of epoch 64, Train Loss: 0.0105740, Test Loss: 0.0246631\n",
      "End of epoch 65, Train Loss: 0.0104459, Test Loss: 0.0243859\n",
      "End of epoch 66, Train Loss: 0.0102299, Test Loss: 0.0248011\n",
      "End of epoch 67, Train Loss: 0.0099374, Test Loss: 0.0246558\n",
      "End of epoch 68, Train Loss: 0.0097435, Test Loss: 0.0251003\n",
      "End of epoch 69, Train Loss: 0.0094243, Test Loss: 0.0246001\n",
      "End of epoch 70, Train Loss: 0.0091608, Test Loss: 0.0246214\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "# training loop\n",
    "learning_rate = 0.00005\n",
    "\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# if the test loss has not improved in the last k epochs, stop training\n",
    "k = 5\n",
    "\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    for idx,batch in enumerate(train_loader):\n",
    "        ids = batch['ids'].to(device, dtype=torch.long)\n",
    "        mask = batch['mask'].to(device, dtype=torch.long)\n",
    "        token_type_ids = batch['token_type_ids'].to(device, dtype=torch.long)\n",
    "        labels = batch['labels'].to(device, dtype=torch.float)\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # backward pass\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # save the losses\n",
    "    train_losses.append(epoch_loss(model, train_loader, criterion))\n",
    "    test_losses.append(epoch_loss(model, test_loader, criterion))\n",
    "\n",
    "    epochs_trained += 1\n",
    "    \n",
    "    print(f'End of epoch {epochs_trained}, Train Loss: {train_losses[-1]:.7f}, Test Loss: {test_losses[-1]:.7f}')\n",
    "\n",
    "    # if the test loss has not improved in the last k epochs break\n",
    "    if len(test_losses) > k and all(test_losses[-(k+1)] < tl for tl in test_losses[-k:]):\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "m-OJN96cmavB"
   },
   "outputs": [],
   "source": [
    "# get the predicitons and corresponding labels\n",
    "def get_pred_true(model, data_loader, D_out):\n",
    "\n",
    "    y_pred = np.zeros((1,D_out))\n",
    "    y_true = np.zeros((1,D_out))\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        for idx,batch in enumerate(data_loader):\n",
    "            ids = batch['ids'].to(device, dtype=torch.long)\n",
    "            mask = batch['mask'].to(device, dtype=torch.long)\n",
    "            token_type_ids = batch['token_type_ids'].to(device, dtype=torch.long)\n",
    "            labels = batch['labels'].to(device, dtype=torch.float)\n",
    "            \n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            \n",
    "            y_pred = np.concatenate((y_pred,outputs.detach().cpu().numpy()), axis=0)\n",
    "            y_true = np.concatenate((y_true,np.array(labels.cpu())), axis=0)\n",
    "            \n",
    "    return y_pred[1:,:], y_true[1:,:]\n",
    "\n",
    "model.eval()  \n",
    "y_pred_temp, y_true = get_pred_true(model, test_loader, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "JtdtWDQAmavC",
    "outputId": "470025f6-e914-4b71-d789-2dfa69bdf8d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./bert/electra-small-swedish-cased-discriminator\n",
      "Number of labels in training set: 186/197 (94.42 %)\n",
      "Number of labels in test set: 140/197 (71.07 %)\n",
      "Number of labels present in both sets: 129/197 (65.48 %)\n",
      "\n",
      "Micro-average F1-score: 0.6536553092954018\n",
      "Weighted-average F1-score: 0.6205014524544166\n",
      "Macro-average F1-score: 0.42898624268906116\n",
      "Accuracy (exact match): 0.3632760898282695\n",
      "Hamming Loss: 0.007020767255195167\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set all values above threshold to 1, else 0\n",
    "th = 0.2\n",
    "y_pred = np.copy(y_pred_temp)\n",
    "y_pred[y_pred > th] = 1\n",
    "y_pred[y_pred <= th] = 0\n",
    "\n",
    "# number of labels in test set\n",
    "#labels_in_test = np.count_nonzero(np.sum(y_true, axis=0))\n",
    "label_counts_test = np.sum(multilab_bin.transform(test_dataset['labels']), axis=0)\n",
    "labels_in_test = np.count_nonzero(label_counts_test)\n",
    "\n",
    "# number of labels in train set\n",
    "label_counts_train = np.sum(multilab_bin.transform(train_dataset['labels']), axis=0)\n",
    "labels_in_train = np.count_nonzero(label_counts_train)\n",
    "\n",
    "# compute size of intersection between labels in train and test\n",
    "label_counts_test[label_counts_test > 0] = 1\n",
    "labels_bin_test = label_counts_test\n",
    "\n",
    "label_counts_train[label_counts_train > 0] = 1\n",
    "labels_bin_train = label_counts_train\n",
    "\n",
    "labels_bin_sum = labels_bin_test + labels_bin_train\n",
    "labels_intersect = np.count_nonzero(labels_bin_sum[labels_bin_sum == 2])\n",
    "\n",
    "print(path_to_bert)\n",
    "print(f'Number of labels in training set: {labels_in_train}/{y_true.shape[1]} ({labels_in_train/y_true.shape[1]*100:.2f} %)')\n",
    "print(f'Number of labels in test set: {labels_in_test}/{y_true.shape[1]} ({labels_in_test/y_true.shape[1]*100:.2f} %)')\n",
    "print(f'Number of labels present in both sets: {labels_intersect}/{y_true.shape[1]} ({labels_intersect/y_true.shape[1]*100:.2f} %)')\n",
    "print()\n",
    "print(f\"Micro-average F1-score: {f1_score(y_true, y_pred, average='micro')}\")\n",
    "print(f\"Weighted-average F1-score: {f1_score(y_true, y_pred, average='weighted', zero_division=1)}\")\n",
    "print(f\"Macro-average F1-score: {f1_score(y_true, y_pred, average='macro', zero_division=1)}\")\n",
    "#print(f\"Sample-average Jaccard score: {jaccard_score(y_true, y_pred, average='samples', zero_division=1)}\")\n",
    "print(f\"Accuracy (exact match): {accuracy_score(y_true, y_pred)}\")\n",
    "print(f\"Hamming Loss: {hamming_loss(y_true, y_pred)}\")\n",
    "\n",
    "x = [x for x in range(1,len(train_losses)+1)]\n",
    "xticks = [x*len(train_losses)//5 for x in range(1,6)]\n",
    "plt.plot(x, test_losses)\n",
    "plt.plot(x, train_losses)\n",
    "plt.legend(['Test Loss', 'Train Loss'])\n",
    "plt.xticks(xticks,xticks)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "#plt.gca().set_ylim([0,0.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aZQ1B-mOmavC",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for i in range(len(test_dataset)):\n",
    "    text = str(test_dataset['text'][i])\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    inputs = tok.encode_plus(\n",
    "        text,\n",
    "        None,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_token_type_ids=True\n",
    "    )\n",
    "    ids = torch.tensor(inputs['input_ids'], dtype=torch.long).view(1,-1).to(device)\n",
    "    mask = torch.tensor(inputs['attention_mask'], dtype=torch.long).view(1,-1).to(device)\n",
    "    token_type_ids = torch.tensor(inputs['token_type_ids'], dtype=torch.long).view(1,-1).to(device)\n",
    "    \n",
    "    y_pred_row = model(ids, mask, token_type_ids).detach().cpu().numpy()\n",
    "    \n",
    "    y_pred_row[y_pred_row > th] = 1\n",
    "    y_pred_row[y_pred_row <= th] = 0\n",
    "    \n",
    "    print(test_dataset['text'][i])\n",
    "    print(f'Prediction: {multilab_bin.inverse_transform(y_pred_row)}')\n",
    "    print(f'Labels: {test_dataset[\"labels\"][i]}')\n",
    "    print('- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "SmnfecMFmavD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "transformer_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
