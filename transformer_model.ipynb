{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "transformer_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReHiq1Llr_7M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "308de602-d8e6-4df4-f7cf-5b0baa15475d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqMdrxYy2iin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e49ba00-e535-411d-b142-6d0815835101"
      },
      "source": [
        "import os\r\n",
        "\r\n",
        "if os.getcwd() == '/content':\r\n",
        "  % cd drive/MyDrive/exjobb_project\r\n",
        "\r\n",
        "print(os.getcwd())\r\n",
        "\r\n",
        "!pip install transformers\r\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/exjobb_project\n",
            "/content/drive/MyDrive/exjobb_project\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.3.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.95)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztQ8dAWOmau7"
      },
      "source": [
        "from transformers import BertModel\n",
        "from transformers import BertTokenizer\n",
        "from transformers import PretrainedConfig\n",
        "from transformers import XLNetModel, XLNetTokenizer\n",
        "\n",
        "# metrics\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import jaccard_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import hamming_loss\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import json\n",
        "import time\n",
        "import regex as re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import UnivariateSpline\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "from torch.nn import BCELoss\n",
        "from collections import Counter\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8gxL6oKgo2a"
      },
      "source": [
        "# use an RNG seed for reproducibility\r\n",
        "seed_list = [2, 4, 8]\r\n",
        "\r\n",
        "SEED = seed_list[2]\r\n",
        "\r\n",
        "# set the RNG seed\r\n",
        "np.random.seed(SEED)\r\n",
        "torch.manual_seed(SEED)\r\n",
        "\r\n",
        "# summary of current run\r\n",
        "summary = f'SEED: {SEED}'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuzjiHVMmau9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5bae47e-133b-40b0-850d-5e4a1b68de4f"
      },
      "source": [
        "path_to_data = './data'\n",
        "\n",
        "# create dataframe from sessions.json\n",
        "df = pd.read_json(f'{path_to_data}/sessions/sessions_03_08.json')\n",
        "df.head()\n",
        "\n",
        "# create dictionaries for switching between symptom and id\n",
        "id2sym = {}\n",
        "sym2id = {}\n",
        "\n",
        "with open(f'{path_to_data}/symptoms/symptoms_03_08.json') as json_file:\n",
        "    data = json.load(json_file)\n",
        "    for sym in data:\n",
        "        id2sym[sym['id']] = sym['name']\n",
        "        sym2id[sym['name']] = sym['id']\n",
        "        \n",
        "print(f'Numer of symptoms in id2sym: {len(sym2id)}')\n",
        "\n",
        "if False:\n",
        "    # remove labels that have less than m occurrences\n",
        "    m = 100\n",
        "\n",
        "    labels_list = df['all_confirmed'].tolist()\n",
        "    labels_list = sum(labels_list, [])\n",
        "    c = Counter(labels_list)\n",
        "    for i in range(len(df)):\n",
        "        to_remove = []\n",
        "        \n",
        "        # find labels that should be removed \n",
        "        for j in range(len(df['all_confirmed'][i])):\n",
        "            if c[df['all_confirmed'][i][j]] < m:\n",
        "                to_remove.append(j)\n",
        "                \n",
        "        # remove the labels\n",
        "        shift = 0\n",
        "        for j in range(len(to_remove)):\n",
        "            df['all_confirmed'][i].pop(to_remove[j]-shift)\n",
        "            shift += 1\n",
        "\n",
        "        \n",
        "# add column with the symptom names\n",
        "sym_names = []\n",
        "\n",
        "for syms in df['confirmed']:\n",
        "    if len(syms) != 0:\n",
        "        sym_names.append([id2sym[x] for x in syms])\n",
        "    else:\n",
        "        sym_names.append([])\n",
        "\n",
        "df['labels'] = sym_names\n",
        "\n",
        "# turn suggested column into symptom names\n",
        "sym_names = []\n",
        "\n",
        "for syms in df['suggested']:\n",
        "    if len(syms) != 0:\n",
        "        sym_names.append([id2sym[x] for x in syms if x in id2sym])\n",
        "    else:\n",
        "        sym_names.append([])\n",
        "\n",
        "df['suggested'] = sym_names\n",
        "    \n",
        "\n",
        "# remove all rows with no confirmed labels\n",
        "df = df[df['confirmed'].map(len) > 0]\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "# remove unused columns\n",
        "df.drop('confirmed', inplace=True, axis=1)\n",
        "df.drop('all_confirmed', inplace=True, axis=1)\n",
        "#df.drop('suggested', inplace=True, axis=1)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Numer of symptoms in id2sym: 459\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9xph3WAmau-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "eb50b526-bbdc-4b9c-f4b7-47ffca6c4700"
      },
      "source": [
        "# use only first limit samples\n",
        "#limit = int(len(df)/2)\n",
        "#limit = 200\n",
        "#df = df[:limit]\n",
        "\n",
        "print(f'Number of samples in df: {len(df)}')\n",
        "\n",
        "if False:\n",
        "    # check if there are duplicates and remove them if found\n",
        "    dup_set = set()\n",
        "    dup_count = 0\n",
        "\n",
        "    for ind, row in df.iterrows():\n",
        "        temp_tup = (row['text'], tuple(row['labels']))\n",
        "        if temp_tup in dup_set:\n",
        "            # remove text if the size is larger than 40\n",
        "            #if len(row['text']) > 40:\n",
        "                #print(f'Dropping index {ind} with text: {row[\"text\"]}')\n",
        "                df.drop(ind, inplace=True)\n",
        "                dup_count += 1\n",
        "        else:\n",
        "            dup_set.add((row['text'], tuple(row['labels'])))\n",
        "    df = df.reset_index(drop=True)\n",
        "\n",
        "    print(f'Removed {dup_count} duplicates')\n",
        "    print(f'Number of samples in df after removal: {len(df)}')\n",
        "\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples in df: 9621\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>suggested</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Slut på medicin.</td>\n",
              "      <td>Man</td>\n",
              "      <td>35</td>\n",
              "      <td>[Nyinsatta läkemedel]</td>\n",
              "      <td>[Känd astma, Känd lungsjukdom]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Jag har hostat snart 3 veckor. Snorar fram och...</td>\n",
              "      <td>Kvinna</td>\n",
              "      <td>37</td>\n",
              "      <td>[Snuva, Hosta]</td>\n",
              "      <td>[Snuva, Hosta]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Gissar att det är hasselallergi. Stora besvär ...</td>\n",
              "      <td>Man</td>\n",
              "      <td>60</td>\n",
              "      <td>[Kissar ofta, Sveda vid miktion, Synliga hudbe...</td>\n",
              "      <td>[Tårande ögon, Hudbesvär, Synliga hudbesvär]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Båda pappa och son hostar</td>\n",
              "      <td>Man</td>\n",
              "      <td>3</td>\n",
              "      <td>[Hosta]</td>\n",
              "      <td>[Hosta, Produktiv hosta]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>H</td>\n",
              "      <td>Kvinna</td>\n",
              "      <td>30</td>\n",
              "      <td>[]</td>\n",
              "      <td>[Hosta]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  ...                                        labels\n",
              "0                                   Slut på medicin.  ...                [Känd astma, Känd lungsjukdom]\n",
              "1  Jag har hostat snart 3 veckor. Snorar fram och...  ...                                [Snuva, Hosta]\n",
              "2  Gissar att det är hasselallergi. Stora besvär ...  ...  [Tårande ögon, Hudbesvär, Synliga hudbesvär]\n",
              "3                          Båda pappa och son hostar  ...                      [Hosta, Produktiv hosta]\n",
              "4                                                  H  ...                                       [Hosta]\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp_L0-yhmau_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d74553d-766b-4d7c-fa05-2a02cfa941d8"
      },
      "source": [
        "# choose which transformer model to use\n",
        "\n",
        "# ELECTRA models\n",
        "#path_to_model = r'./bert/electra-small-swedish-cased-discriminator'\n",
        "#path_to_model = r'./bert/electra-base-swedish-cased-discriminator'\n",
        "\n",
        "# BERT models\n",
        "#path_to_model = r'./bert/bert-base-cased'\n",
        "path_to_model = r'./bert/bert-base-swedish-cased'\n",
        "#path_to_model = r'./bert/bert-base-multilingual-cased'\n",
        "\n",
        "# XLNet models\n",
        "#path_to_model = r'./xlnet/xlnet-base-cased'\n",
        "\n",
        "print(f'Model path: {path_to_model}')\n",
        "summary += f'\\nModel path: {path_to_model}'\n",
        "\n",
        "tok = None\n",
        "if re.search(r'xlnet', path_to_model):\n",
        "    tok = XLNetTokenizer.from_pretrained(path_to_model)\n",
        "    print('Using XLNetTokenizer')\n",
        "    summary += f'\\nTokenizer: XLNetTokenizer'\n",
        "elif re.search(r'bert', path_to_model):\n",
        "    tok = BertTokenizer.from_pretrained(path_to_model, do_lower_case=False)\n",
        "    print('Using BertTokenizer')\n",
        "    summary += f'\\nTokenizer: BertTokenizer'\n",
        "\n",
        "# train a multilabel_binarizer on the labels\n",
        "labels = df['labels'].tolist()\n",
        "multilab_bin = MultiLabelBinarizer()\n",
        "multilab_bin.fit(labels)\n",
        "summary += f'\\nNumber of labels: {len(multilab_bin.classes_)}'\n",
        "\n",
        "# create a one-hot encoding of the gender\n",
        "def gen2vec(cur_gen):\n",
        "    gens = [0, 0, 0]\n",
        "    if cur_gen == 'Kvinna':\n",
        "        gens[0] = 1\n",
        "    elif cur_gen == 'Man':\n",
        "        gens[1] = 1\n",
        "    elif cur_gen == 'Annat':\n",
        "        gens[2] = 1\n",
        "    else:\n",
        "        print(f'UNEXPECTED OPTION FOUND: {cur_gen}')\n",
        "    return gens\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, multilab_bin, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.multilab_bin = multilab_bin\n",
        "        self.data = dataframe\n",
        "        self.text = self.data['text']\n",
        "        self.ages = self.data['age']\n",
        "        self.gens = self.data['sex']\n",
        "        self.labels = self.data['labels']\n",
        "        self.max_len = max_len\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.text[index])\n",
        "        text = ' '.join(text.split())\n",
        "        \n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs['token_type_ids']\n",
        "\n",
        "        # create a three-dimensional one-hot encoded vector to represent gender\n",
        "        cur_gen = self.gens[index]\n",
        "        gens = gen2vec(cur_gen)\n",
        "\n",
        "        # represent age as float\n",
        "        ages = [self.ages[index]]\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'gens': torch.tensor(gens, dtype=torch.long),\n",
        "            'ages': torch.tensor(ages, dtype=torch.float),\n",
        "            #'labels': torch.tensor(np.sum(self.multilab_bin.transform([self.labels[index]]), axis=0), dtype=torch.float)\n",
        "            'labels': torch.tensor(self.multilab_bin.transform([self.labels[index]]).reshape(-1,), dtype=torch.float)\n",
        "        }        "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model path: ./bert/bert-base-swedish-cased\n",
            "Using BertTokenizer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqxfp2B8mau_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "24b5e18f-696c-496a-c9e6-bf3816a233fb"
      },
      "source": [
        "batch_size = 16\n",
        "summary += f'\\nBatch size: {batch_size}'\n",
        "\n",
        "\n",
        "print(f'Tokenizer from {path_to_model}')\n",
        "# max number of tokens in token sequence\n",
        "max_len = 0\n",
        "\n",
        "from scipy.stats import expon\n",
        "# get list of all token sequence lengths\n",
        "seq_lengths = np.zeros((len(df)))\n",
        "\n",
        "for i in range(len(df['text'])):\n",
        "    text = df['text'][i]\n",
        "    inputs = tok.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            padding=False,\n",
        "            return_token_type_ids=True\n",
        "            )\n",
        "    \n",
        "    seq_lengths[i] = len(inputs['input_ids'])\n",
        "\n",
        "    if len(inputs['input_ids']) >= max_len and False:\n",
        "        print(len(inputs['input_ids']))\n",
        "        print(df['text'][i])\n",
        "        print(inputs['input_ids'])\n",
        "        print(tok.convert_ids_to_tokens(inputs['input_ids']))\n",
        "        max_len = len(inputs['input_ids'])\n",
        "\n",
        "# fit a normal distribution to the sequence lengths\n",
        "loc, scale = expon.fit(seq_lengths)\n",
        "\n",
        "# compute [perc]th percentile\n",
        "perc = 95\n",
        "max_len = int(expon.ppf(0.01*perc, loc, scale) + 1) # +1 to round up\n",
        "\n",
        "# plot histogram of token sequence lengths\n",
        "plt.hist(seq_lengths, bins=25, density=True, alpha=0.6, color='g')\n",
        "\n",
        "# plot the PDF\n",
        "xmin, xmax = plt.xlim()\n",
        "x = np.linspace(loc, xmax, 100)\n",
        "p = expon.pdf(x, loc, scale)\n",
        "plt.plot(x, p, 'k', linewidth=2)\n",
        "plt.xlabel('Token sequence length')\n",
        "title = f'Fit results: loc = {loc:.2f}, scale = {scale:.2f}'\n",
        "plt.title(title)\n",
        "plt.show()\n",
        "\n",
        "# compute mean and variance of distribution\n",
        "mean = expon.mean(loc, scale)\n",
        "std = expon.std(loc, scale)\n",
        "\n",
        "print(f'Mean = {mean:.2f}, Std = {std:.2f}')\n",
        "print(f'Token sequence length including {perc}% of all sequences: {max_len}')\n",
        "\n",
        "# TEMP: Manually set the max_len\n",
        "#max_len = 100\n",
        "\n",
        "print(f'Max token sequence length: {max_len}\\n')\n",
        "summary += f'\\nMax token sequence length: {max_len}'\n",
        "\n",
        "\n",
        "\n",
        "# prepare dataloaders\n",
        "train_dataset, test_dataset = train_test_split(df,\n",
        "                                        test_size=0.2,\n",
        "                                        shuffle=True)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "test_dataset = test_dataset.reset_index(drop=True)\n",
        "\n",
        "train_set = CustomDataset(train_dataset, tok, multilab_bin, max_len)\n",
        "test_set = CustomDataset(test_dataset, tok, multilab_bin, max_len)\n",
        "\n",
        "train_params = {'batch_size': batch_size,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "               }\n",
        "test_params = {'batch_size': batch_size,\n",
        "               'shuffle': True,\n",
        "               'num_workers': 0\n",
        "              }\n",
        "\n",
        "train_loader = DataLoader(train_set, **train_params)\n",
        "test_loader = DataLoader(test_set, **test_params)\n",
        "\n",
        "print(f'Train set: {len(train_dataset)} samples')\n",
        "print(f'Test set: {len(test_dataset)} samples')\n",
        "\n",
        "summary += f'\\nTraining set: {len(train_dataset)} samples\\nTest set: {len(test_dataset)} samples'"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizer from ./bert/bert-base-swedish-cased\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Mean = 34.76, Std = 31.76\n",
            "Token sequence length including 95% of all sequences: 99\n",
            "Max token sequence length: 99\n",
            "\n",
            "Train set: 7696 samples\n",
            "Test set: 1925 samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHjybrQHmau_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f21b5d8-084a-4637-8af3-f66b06809830"
      },
      "source": [
        "# decide which device to use. use cuda if available\n",
        "dev = ''\n",
        "if torch.cuda.is_available():\n",
        "    dev = 'cuda:0'\n",
        "    summary += f'\\nCUDA device: {torch.cuda.get_device_name(0)}'\n",
        "else:\n",
        "    dev = 'cpu'\n",
        "    summary += '\\nCUDA device: None'\n",
        "\n",
        "print(f'dev = {dev}')\n",
        "print(f'Number of available GPUs: {torch.cuda.device_count()}')\n",
        "\n",
        "# print the device names\n",
        "for i in range(torch.cuda.device_count()):\n",
        "  print(f'Device {i}: {torch.cuda.get_device_name(i)}')\n",
        "\n",
        "device = torch.device(dev)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dev = cuda:0\n",
            "Number of available GPUs: 1\n",
            "Device 0: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBYDTZNvmavA"
      },
      "source": [
        "class BERTClass(nn.Module):\n",
        "    def __init__(self, config, path_to_bert, output_dim):\n",
        "        super(BERTClass, self).__init__()\n",
        "        #config = PretrainedConfig.from_json_file(f'{path_to_bert}/config.json')\n",
        "        self.bert = BertModel.from_pretrained(path_to_bert)\n",
        "        #self.bert = BertModel(config)\n",
        "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.fc = nn.Linear(config.hidden_size, output_dim)\n",
        "        self.sigm = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        x = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
        "        x = self.dropout(x[1])\n",
        "        x = self.fc(x)\n",
        "        output = self.sigm(x)\n",
        "        return output\n",
        "\n",
        "# a BERTClass that also takes in additional features aside from text\n",
        "class BERTClass_Plus(nn.Module):\n",
        "    def __init__(self, config, path_to_bert, output_dim, extra_features):\n",
        "        super(BERTClass_Plus, self).__init__()\n",
        "        self.num_extra_features = extra_features # number of extra features\n",
        "        self.bert = BertModel.from_pretrained(path_to_bert)\n",
        "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.fc = nn.Linear(config.hidden_size+self.num_extra_features, output_dim)\n",
        "        self.sigm = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, ids, mask, token_type_ids, ages, gens):\n",
        "        x = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
        "        x = self.dropout(x[1])\n",
        "        \n",
        "        # add extra features through concatenation\n",
        "        x = torch.cat([x,ages,gens], dim=1)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        output = self.sigm(x)\n",
        "        return output\n",
        "\n",
        "class XLNetClass(nn.Module):\n",
        "    def __init__(self, config, path_to_model, output_dim):\n",
        "        super(XLNetClass, self).__init__()\n",
        "        self.transformer = XLNetModel.from_pretrained(path_to_model)\n",
        "\n",
        "        # - - - - Imitate BertPooler - - - -\n",
        "        #self.dense = nn.Linear(config.d_model, config.d_model)\n",
        "        #self.tanh = nn.Tanh()\n",
        "        # - - - - - - - - - - - - - - - - - - -\n",
        "\n",
        "        self.dropout = nn.Dropout(config.summary_last_dropout)\n",
        "\n",
        "        self.fc = nn.Linear(config.d_model, output_dim)\n",
        "        self.sigm = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        x = self.transformer(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
        "\n",
        "        # - - - - Imitate BertPooler - - - -\n",
        "        x = x[0][:, -1] # use the CLS token\n",
        "        #x = self.dense(x)\n",
        "        #x = self.tanh(x)\n",
        "        # - - - - - - - - - - - - - - - - - -\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        output = self.sigm(x)\n",
        "        return output\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGGhVr_tmavA"
      },
      "source": [
        "# compute the loss of an epoch by averaging all batch losses\n",
        "def epoch_loss(model, data_loader, criterion):\n",
        "    loss = 0\n",
        "    batch_count = 0\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for idx,batch in enumerate(data_loader):\n",
        "            ids = batch['ids'].to(device, dtype=torch.long)\n",
        "            mask = batch['mask'].to(device, dtype=torch.long)\n",
        "            token_type_ids = batch['token_type_ids'].to(device, dtype=torch.long)\n",
        "            gens = batch['gens'].to(device, dtype=torch.long)\n",
        "            ages = batch['ages'].to(device, dtype=torch.float)\n",
        "            labels = batch['labels'].to(device, dtype=torch.float)\n",
        "\n",
        "            # forward pass            \n",
        "            #outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "            # new forward pass with extra features\n",
        "            outputs = model(ids, mask, token_type_ids, ages, gens)\n",
        "\n",
        "            loss += criterion(outputs, labels)\n",
        "            batch_count += 1\n",
        "    model.train()\n",
        "    return loss / batch_count"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gr7rj2zwWj0K",
        "outputId": "4cf8cb16-acdc-4862-a232-d6093707a0b6"
      },
      "source": [
        "# define the model\r\n",
        "D_out = len(multilab_bin.classes_)\r\n",
        "\r\n",
        "config = PretrainedConfig.from_json_file(f'{path_to_model}/config.json')\r\n",
        "\r\n",
        "# choose appropriate nn.module class based on pathname\r\n",
        "model = None\r\n",
        "if re.search(r'xlnet', path_to_model):\r\n",
        "    model = XLNetClass(config, path_to_model, D_out)\r\n",
        "    summary += '\\nUModel class: XLNetClass'\r\n",
        "elif re.search(r'bert', path_to_model):\r\n",
        "    #model = BERTClass(config, path_to_model, D_out)\r\n",
        "    model = BERTClass_Plus(config, path_to_model, D_out, 4) # BERTClass with additional features\r\n",
        "    summary += '\\nModel class: BertClass'\r\n",
        "\r\n",
        "model.train()\r\n",
        "model.to(device)\r\n",
        "\r\n",
        "# number of epochs trained\r\n",
        "epochs_trained = 0\r\n",
        "\r\n",
        "# losses over entire train-/test-set per epoch\r\n",
        "train_losses = []\r\n",
        "test_losses = []\r\n",
        "\r\n",
        "print(summary)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SEED: 8\n",
            "Model path: ./bert/bert-base-swedish-cased\n",
            "Tokenizer: BertTokenizer\n",
            "Number of labels: 230\n",
            "Batch size: 16\n",
            "Max token sequence length: 99\n",
            "Training set: 7696 samples\n",
            "Test set: 1925 samples\n",
            "CUDA device: Tesla T4\n",
            "Model class: BertClass\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bs7jzf1amavA",
        "scrolled": false
      },
      "source": [
        "\n",
        "model.train()\n",
        "\n",
        "# training loop\n",
        "learning_rate = 5e-5\n",
        "#learning_rate = 1e-5 # 1e-5 works better for bert-multilingual\n",
        "\n",
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# if the test loss has not improved in the last k epochs, stop training\n",
        "k = 5\n",
        "\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    # keep track of time taken per epoch\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx,batch in enumerate(train_loader):\n",
        "        ids = batch['ids'].to(device, dtype=torch.long)\n",
        "        mask = batch['mask'].to(device, dtype=torch.long)\n",
        "        token_type_ids = batch['token_type_ids'].to(device, dtype=torch.long)\n",
        "        gens = batch['gens'].to(device, dtype=torch.long)\n",
        "        ages = batch['ages'].to(device, dtype=torch.float)\n",
        "        labels = batch['labels'].to(device, dtype=torch.float)\n",
        "\n",
        "        # forward pass\n",
        "        #outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "        # new forward pass with extra features\n",
        "        outputs = model(ids, mask, token_type_ids, ages, gens)\n",
        "        \n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # backward pass\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    # save the losses\n",
        "    train_losses.append(epoch_loss(model, train_loader, criterion))\n",
        "    test_losses.append(epoch_loss(model, test_loader, criterion))\n",
        "\n",
        "    epochs_trained += 1\n",
        "    \n",
        "    print(f'End of epoch {epochs_trained}, Train Loss: {train_losses[-1]:.7f}, Test Loss: {test_losses[-1]:.7f}, Time elapsed: {time.time()-start_time:.2f} s')\n",
        "\n",
        "    # if the test loss has not improved in the last k epochs break\n",
        "    if len(test_losses) > k and all(test_losses[-(k+1)] < tl for tl in test_losses[-k:]):\n",
        "      print(f'Test loss has not improved in the last {k} epochs. Stopping ...')\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-OJN96cmavB"
      },
      "source": [
        "# get the predicitons and corresponding labels\n",
        "def get_pred_true(model, data_loader, D_out):\n",
        "\n",
        "    y_pred = np.zeros((1,D_out))\n",
        "    y_true = np.zeros((1,D_out))\n",
        "  \n",
        "    with torch.no_grad():\n",
        "        for idx,batch in enumerate(data_loader):\n",
        "            ids = batch['ids'].to(device, dtype=torch.long)\n",
        "            mask = batch['mask'].to(device, dtype=torch.long)\n",
        "            token_type_ids = batch['token_type_ids'].to(device, dtype=torch.long)\n",
        "            labels = batch['labels'].to(device, dtype=torch.float)\n",
        "            \n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "            \n",
        "            y_pred = np.concatenate((y_pred,outputs.detach().cpu().numpy()), axis=0)\n",
        "            y_true = np.concatenate((y_true,np.array(labels.cpu())), axis=0)\n",
        "            \n",
        "    return y_pred[1:,:], y_true[1:,:]\n",
        "\n",
        "# get the predictions, labels, and matches\n",
        "def get_pred_true_match(model, dataset, D_out):\n",
        "\n",
        "    y_pred = np.zeros((1,D_out))\n",
        "    y_true = np.zeros((1,D_out))\n",
        "    y_match = np.zeros((1,D_out))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(dataset)):\n",
        "            text = str(dataset['text'][i])\n",
        "            text = ' '.join(text.split())\n",
        "            \n",
        "            inputs = tok.encode_plus(\n",
        "                text,\n",
        "                None,\n",
        "                add_special_tokens=True,\n",
        "                max_length=max_len,\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                return_token_type_ids=True\n",
        "            )\n",
        "\n",
        "            ids = torch.tensor(inputs['input_ids'], dtype=torch.long).view(1,-1).to(device)\n",
        "            mask = torch.tensor(inputs['attention_mask'], dtype=torch.long).view(1,-1).to(device)\n",
        "            token_type_ids = torch.tensor(inputs['token_type_ids'], dtype=torch.long).view(1,-1).to(device)\n",
        "            \n",
        "            gens = torch.tensor(gen2vec(dataset['sex'][i]), dtype=torch.long).view(1,-1).to(device)\n",
        "            ages = torch.tensor([dataset['age'][i]], dtype=torch.float).view(1,-1).to(device)\n",
        "            \n",
        "            # forward pass\n",
        "            #outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "            # new forward pass with extra features\n",
        "            outputs = model(ids, mask, token_type_ids, ages, gens)\n",
        "\n",
        "            # get the labels\n",
        "            labels = dataset['labels'][i]\n",
        "            labels = np.array(multilab_bin.transform([labels]))\n",
        "\n",
        "            # get the matches\n",
        "            # matches = [id2sym[x] for x in dataset['suggested'][i] if x in id2sym]\n",
        "            matches = dataset['suggested'][i]\n",
        "            matches = np.array(multilab_bin.transform([matches]))\n",
        "\n",
        "            y_pred = np.concatenate((y_pred,outputs.detach().cpu().numpy()), axis=0)\n",
        "            y_true = np.concatenate((y_true, labels), axis=0)\n",
        "            y_match = np.concatenate((y_match, matches), axis=0)\n",
        "            \n",
        "    print(y_true.shape)\n",
        "    return y_pred[1:,:], y_true[1:,:], y_match[1:,:]\n",
        "\n",
        "model.eval()  \n",
        "#y_pred_temp, y_true = get_pred_true(model, test_loader, D_out)\n",
        "y_pred_temp, y_true, y_match = get_pred_true_match(model, test_dataset, D_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtdtWDQAmavC"
      },
      "source": [
        "# set all values above threshold to 1, else 0\n",
        "th = 0.5\n",
        "y_pred = np.copy(y_pred_temp)\n",
        "y_pred[y_pred > th] = 1\n",
        "y_pred[y_pred <= th] = 0\n",
        "\n",
        "# TEMPORARY: remove all guesses for 'Hudbesvär' to get comparable results to match\n",
        "#rem_idx = -1\n",
        "#for i in range(len(multilab_bin.classes_)):\n",
        "#    if multilab_bin.classes_[i] == 'Hudbesvär':\n",
        "#        rem_idx = i\n",
        "#y_pred[:,rem_idx] = 0\n",
        "\n",
        "# number of labels in test set\n",
        "#labels_in_test = np.count_nonzero(np.sum(y_true, axis=0))\n",
        "label_counts_test = np.sum(multilab_bin.transform(test_dataset['labels']), axis=0)\n",
        "labels_in_test = np.count_nonzero(label_counts_test)\n",
        "\n",
        "# number of labels in train set\n",
        "label_counts_train = np.sum(multilab_bin.transform(train_dataset['labels']), axis=0)\n",
        "labels_in_train = np.count_nonzero(label_counts_train)\n",
        "\n",
        "# compute size of intersection between labels in train and test\n",
        "label_counts_test[label_counts_test > 0] = 1\n",
        "labels_bin_test = label_counts_test\n",
        "\n",
        "label_counts_train[label_counts_train > 0] = 1\n",
        "labels_bin_train = label_counts_train\n",
        "\n",
        "labels_bin_sum = labels_bin_test + labels_bin_train\n",
        "labels_intersect = np.count_nonzero(labels_bin_sum[labels_bin_sum == 2])\n",
        "\n",
        "print(f'SEED: {SEED}')\n",
        "print(path_to_model)\n",
        "print(f'Number of labels in training set: {labels_in_train}/{y_true.shape[1]} ({labels_in_train/y_true.shape[1]*100:.2f} %)')\n",
        "print(f'Number of labels in test set: {labels_in_test}/{y_true.shape[1]} ({labels_in_test/y_true.shape[1]*100:.2f} %)')\n",
        "print(f'Number of labels present in both sets: {labels_intersect}/{y_true.shape[1]} ({labels_intersect/y_true.shape[1]*100:.2f} %)')\n",
        "print()\n",
        "print(f\"Micro-average F1-score: {f1_score(y_true, y_pred, average='micro')}\")\n",
        "print(f\"Weighted-average F1-score: {f1_score(y_true, y_pred, average='weighted', zero_division=1)}\")\n",
        "print(f\"Macro-average F1-score: {f1_score(y_true, y_pred, average='macro', zero_division=1)}\")\n",
        "#print(f\"Sample-average Jaccard score: {jaccard_score(y_true, y_pred, average='samples', zero_division=1)}\")\n",
        "print(f\"Accuracy (exact match): {accuracy_score(y_true, y_pred)}\")\n",
        "print(f\"Hamming Loss: {hamming_loss(y_true, y_pred)}\")\n",
        "\n",
        "print('\\nMatching system metrics:')\n",
        "print(f\"Micro-average F1-score: {f1_score(y_true, y_match, average='micro')}\")\n",
        "print(f\"Weighted-average F1-score: {f1_score(y_true, y_match, average='weighted', zero_division=1)}\")\n",
        "print(f\"Macro-average F1-score: {f1_score(y_true, y_match, average='macro', zero_division=1)}\")\n",
        "#print(f\"Sample-average Jaccard score: {jaccard_score(y_true, y_pred, average='samples', zero_division=1)}\")\n",
        "print(f\"Accuracy (exact match): {accuracy_score(y_true, y_match)}\")\n",
        "print(f\"Hamming Loss: {hamming_loss(y_true, y_match)}\")\n",
        "\n",
        "x = [x for x in range(1,len(train_losses)+1)]\n",
        "xticks = [x*len(train_losses)//5 for x in range(1,6)]\n",
        "plt.plot(x, test_losses)\n",
        "plt.plot(x, train_losses)\n",
        "plt.legend(['Test Loss', 'Train Loss'])\n",
        "plt.xticks(xticks,xticks)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "# plot f1-score in relation to label frequency in training set\n",
        "f1_per_label = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
        "label_counts_train = np.sum(multilab_bin.transform(train_dataset['labels']), axis=0)\n",
        "label_counts_test = np.sum(multilab_bin.transform(test_dataset['labels']), axis=0)\n",
        "\n",
        "# plot f1-score for the matching system\n",
        "f1_per_label_match = f1_score(y_true, y_match, average=None, zero_division=0)\n",
        "\n",
        "# also plot recall in relation to label frequency in training set\n",
        "recall_per_label = recall_score(y_true, y_pred, average=None, zero_division=0)\n",
        "\n",
        "# and recall per label for the matching algorithm\n",
        "recall_per_label_match = recall_score(y_true, y_match, average=None, zero_division=0)\n",
        "\n",
        "# only include labels that are present in the test set\n",
        "zipped = [(f1_per_label[i], f1_per_label_match[i], recall_per_label[i], recall_per_label_match[i], multilab_bin.classes_[i], label_counts_test[i], label_counts_train[i]) for i in range(len(label_counts_test)) if label_counts_test[i] > 0]\n",
        "zipped = sorted(zipped, key=lambda tup: tup[-1])\n",
        "\n",
        "f1_per_label = [zipped[i][0] for i in range(len(zipped))]\n",
        "f1_per_label_match = [zipped[i][1] for i in range(len(zipped))]\n",
        "recall_per_label = [zipped[i][2] for i in range(len(zipped))]\n",
        "recall_per_label_match = [zipped[i][3] for i in range(len(zipped))]\n",
        "label_counts_train = [zipped[i][-1] for i in range(len(zipped))]\n",
        "print(zipped[-1][3])\n",
        "\n",
        "# plot f1-score in relation to label frequency in training set\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "plt.plot(label_counts_train, f1_per_label)\n",
        "plt.plot(label_counts_train, f1_per_label, '.')\n",
        "plt.plot(label_counts_train, f1_per_label_match, alpha=0.6)\n",
        "plt.plot(label_counts_train, f1_per_label_match, '.', alpha=0.6)\n",
        "ax.set_xscale('log')\n",
        "ax.set_ylim((0,1))\n",
        "#ax.set_xlim((50,3000))\n",
        "plt.xlabel('Label frequency in training set')\n",
        "plt.ylabel('F1-score')\n",
        "plt.grid(True, which='both', linestyle=':')\n",
        "plt.show()\n",
        "\n",
        "# plot recall in relation to label frequency in training set\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "plt.plot(label_counts_train, recall_per_label)\n",
        "plt.plot(label_counts_train, recall_per_label, '.')\n",
        "plt.plot(label_counts_train, recall_per_label_match)\n",
        "plt.plot(label_counts_train, recall_per_label_match, '.')\n",
        "ax.set_xscale('log')\n",
        "ax.set_ylim((0,1))\n",
        "plt.xlabel('Label frequency in training set')\n",
        "plt.ylabel('Recall')\n",
        "plt.grid(True, which='both', linestyle=':')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lf0Rhz2q9KV_"
      },
      "source": [
        "# write custom text for testing out the model\r\n",
        "if False:\r\n",
        "    text = ''\r\n",
        "\r\n",
        "    inputs = tok.encode_plus(\r\n",
        "        text,\r\n",
        "        None,\r\n",
        "        add_special_tokens=True,\r\n",
        "        max_length=max_len,\r\n",
        "        truncation=True,\r\n",
        "        padding='max_length',\r\n",
        "        return_token_type_ids=True\r\n",
        "    )\r\n",
        "    ids = torch.tensor(inputs['input_ids'], dtype=torch.long).view(1,-1).to(device)\r\n",
        "    mask = torch.tensor(inputs['attention_mask'], dtype=torch.long).view(1,-1).to(device)\r\n",
        "    token_type_ids = torch.tensor(inputs['token_type_ids'], dtype=torch.long).view(1,-1).to(device)\r\n",
        "\r\n",
        "    # get the prediction\r\n",
        "    y_pred_row = model(ids, mask, token_type_ids).detach().cpu()\r\n",
        "\r\n",
        "    y_pred_row[y_pred_row > th] = 1\r\n",
        "    y_pred_row[y_pred_row <= th] = 0\r\n",
        "\r\n",
        "    # print symptoms\r\n",
        "    print(multilab_bin.inverse_transform(y_pred_row))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZQ1B-mOmavC",
        "scrolled": true
      },
      "source": [
        "model.eval()\n",
        "\n",
        "# get a list of tuples containing the samples sorted by loss\n",
        "sorted_samples = []\n",
        "\n",
        "for i in range(len(test_dataset)):\n",
        "    text = str(test_dataset['text'][i])\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    inputs = tok.encode_plus(\n",
        "        text,\n",
        "        None,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_len,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        return_token_type_ids=True\n",
        "    )\n",
        "    ids = torch.tensor(inputs['input_ids'], dtype=torch.long).view(1,-1).to(device)\n",
        "    mask = torch.tensor(inputs['attention_mask'], dtype=torch.long).view(1,-1).to(device)\n",
        "    token_type_ids = torch.tensor(inputs['token_type_ids'], dtype=torch.long).view(1,-1).to(device)\n",
        "    \n",
        "    gens = torch.tensor(gen2vec(test_dataset['sex'][i]), dtype=torch.long).view(1,-1).to(device)\n",
        "    ages = torch.tensor([test_dataset['age'][i]], dtype=torch.float).view(1,-1).to(device)\n",
        "    \n",
        "    # get the prediction\n",
        "    #y_pred_row = model(ids, mask, token_type_ids).detach().cpu()\n",
        "\n",
        "    # new forward pass with extra features\n",
        "    y_pred_row = model(ids, mask, token_type_ids, ages, gens).detach().cpu()\n",
        "\n",
        "\n",
        "    # get the true labels\n",
        "    y_true_row = torch.tensor(multilab_bin.transform([test_dataset['labels'][i]]), dtype=torch.float)\n",
        "\n",
        "    # get the loss\n",
        "    loss = criterion(y_pred_row, y_true_row)\n",
        "\n",
        "    # convert predictions according to threshold\n",
        "    y_pred_row[y_pred_row > th] = 1\n",
        "    y_pred_row[y_pred_row <= th] = 0\n",
        "\n",
        "    # get the suggested labels\n",
        "    y_suggested_row = test_dataset['suggested'][i]\n",
        "\n",
        "    sorted_samples.append((y_pred_row, y_true_row, y_suggested_row, text, loss))\n",
        "\n",
        "# sort samples with regard to loss\n",
        "sorted_samples = sorted(sorted_samples, key=lambda tup: tup[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmnfecMFmavD"
      },
      "source": [
        "# print the m percent best/worst predictions based on loss\r\n",
        "m = 50\r\n",
        "best_worst = 'worst'\r\n",
        "\r\n",
        "nbr_elements = (len(test_dataset) * m) // 100\r\n",
        "\r\n",
        "top_m = []\r\n",
        "if best_worst == 'best':\r\n",
        "  top_m = sorted_samples[:nbr_elements]\r\n",
        "elif best_worst == 'worst':\r\n",
        "  top_m = sorted_samples[len(sorted_samples)-nbr_elements:]\r\n",
        "else:\r\n",
        "  print('Please choose either \"best\" or \"worst\"')\r\n",
        "\r\n",
        "for i in range(len(top_m)):\r\n",
        "  tup = top_m[i]\r\n",
        "  pred = multilab_bin.inverse_transform(tup[0])\r\n",
        "  labels = multilab_bin.inverse_transform(tup[1])\r\n",
        "  suggested = tup[2]#[id2sym[j] for j in tup[2] if j in id2sym]\r\n",
        "  text = tup[3]\r\n",
        "  loss = tup[-1]\r\n",
        "\r\n",
        "  print(f'Loss: {loss}')\r\n",
        "  print(f'{text}')\r\n",
        "  print(f'Prediction: {pred}')\r\n",
        "  print(f'Labels: {labels}')\r\n",
        "  print(f'Suggested: {suggested}')\r\n",
        "  print('- - - - - - - - - - - - - - - - - - - - - - - - -')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1wAgyf_r9wl"
      },
      "source": [
        "# checkout the tokenizer\r\n",
        "max_len_temp = 20\r\n",
        "\r\n",
        "text = 'Plants perform photosynthesis.'\r\n",
        "\r\n",
        "inputs = tok.encode_plus(\r\n",
        "            text,\r\n",
        "            None,\r\n",
        "            add_special_tokens=True,\r\n",
        "            #max_length=max_len,\r\n",
        "            #truncation=True,\r\n",
        "            #padding='max_length',\r\n",
        "            return_token_type_ids=True\r\n",
        "        )\r\n",
        "\r\n",
        "print(inputs['input_ids'])\r\n",
        "print(inputs['token_type_ids'])\r\n",
        "print(inputs['attention_mask'])\r\n",
        "print(tok.convert_ids_to_tokens(inputs['input_ids']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGprW7MQZZ3B"
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    }
  ]
}